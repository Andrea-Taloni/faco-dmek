{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ff30ffd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% Cell 1: Enhanced Imports for Better Optimization\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split, KFold, cross_val_score, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler, RobustScaler\n",
    "from sklearn.linear_model import Ridge, Lasso, ElasticNet, HuberRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, VotingRegressor\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "from sklearn.gaussian_process.kernels import Matern\n",
    "from sklearn.metrics import mean_absolute_error, r2_score\n",
    "from sklearn.pipeline import Pipeline\n",
    "from scipy.optimize import minimize, differential_evolution, dual_annealing\n",
    "from scipy import stats\n",
    "from sklearn.feature_selection import SelectKBest, f_regression, RFE\n",
    "from sklearn.impute import KNNImputer\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# CONFIGURABLE RANDOM SEED - CHANGE THIS TO GET DIFFERENT RESULTS\n",
    "RANDOM_SEED = 123  # Change to any integer for different random splits/results\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(RANDOM_SEED)\n",
    "\n",
    "print(\"Enhanced libraries imported successfully!\")\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(f\"RANDOM SEED SET TO: {RANDOM_SEED}\")\n",
    "print(\"Change RANDOM_SEED value to get different train/test splits and results\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8541af9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% Cell 2: Load Data with Proper Feature Engineering\n",
    "# Load the Excel file\n",
    "df = pd.read_excel('FacoDMEK.xlsx', sheet_name='Cleaned Data')\n",
    "\n",
    "# Display basic information\n",
    "print(f\"Dataset shape: {df.shape}\")\n",
    "print(f\"\\nColumns: {df.columns.tolist()}\")\n",
    "\n",
    "# Calculate derived features\n",
    "df['K_avg_Kerato'] = (df['Keratometric Ks'] + df['Keratometric Kf']) / 2\n",
    "df['K_Astigmatism_Kerato'] = df['Keratometric Ks'] - df['Keratometric Kf']\n",
    "df['Post_Ant_Ratio'] = df['Posterior Km'] / df['Anterior Km']\n",
    "df['AL_K_Ratio'] = df['Bio-AL'] / df['K_avg_Kerato']\n",
    "\n",
    "# Domain-specific features WITHOUT arbitrary normalization\n",
    "df['K_Cylinder'] = np.abs(df['K_Astigmatism_Kerato'])\n",
    "df['AL_Category'] = pd.cut(df['Bio-AL'], bins=[0, 22, 24.5, 26, 100], \n",
    "                           labels=['short', 'normal', 'long', 'very_long'])\n",
    "\n",
    "# ALL normalization will be done later using StandardScaler or RobustScaler\n",
    "# No assumptions about \"average\" values!\n",
    "\n",
    "print(f\"\\nFeature engineering completed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41d02bf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% Cell 3: Improved Train/Test Split Strategy\n",
    "# CRITICAL: Use less stringent requirements to keep more data\n",
    "required_cols = ['Bio-AL', 'K_avg_Kerato', 'IOL Power', \n",
    "                 'PostOP Spherical Equivalent', 'A-Constant']\n",
    "df_complete = df[df[required_cols].notna().all(axis=1)].copy()\n",
    "\n",
    "print(f\"Complete cases for analysis: {len(df_complete)} out of {len(df)}\")\n",
    "\n",
    "# IMPROVED: Use 70/30 split for more test data (better significance testing)\n",
    "# Also stratify by AL category for better representation\n",
    "df_train_val, df_test = train_test_split(\n",
    "    df_complete, \n",
    "    test_size=0.30,  # Increased from 0.20\n",
    "    random_state=RANDOM_SEED,  # USE VARIABLE SEED\n",
    "    stratify=df_complete['AL_Category']\n",
    ")\n",
    "\n",
    "print(f\"\\nTrain/Validation set: {len(df_train_val)} eyes (70%)\")\n",
    "print(f\"Test set: {len(df_test)} eyes (30%)\")\n",
    "print(f\"\\nThis gives us more test data for reliable significance testing\")\n",
    "print(f\"Using random seed: {RANDOM_SEED}\")\n",
    "\n",
    "# Check distribution\n",
    "print(\"\\nAL distribution in sets:\")\n",
    "print(\"Train:\", df_train_val['AL_Category'].value_counts().sort_index())\n",
    "print(\"Test:\", df_test['AL_Category'].value_counts().sort_index())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67472663",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% Cell 4: Enhanced SRK/T Implementation with Constraints\n",
    "def calculate_SRKT(AL, K, A_const, nc=1.333):\n",
    "    \"\"\"Standard SRK/T formula\"\"\"\n",
    "    if pd.isna(AL) or pd.isna(K) or pd.isna(A_const) or K <= 0 or AL <= 0:\n",
    "        return np.nan\n",
    "    \n",
    "    try:\n",
    "        na = 1.336\n",
    "        r = 337.5 / K\n",
    "        \n",
    "        if AL > 24.2:\n",
    "            LCOR = -3.446 + 1.716 * AL - 0.0237 * AL**2\n",
    "        else:\n",
    "            LCOR = AL\n",
    "        \n",
    "        Cw = -5.41 + 0.58412 * LCOR + 0.098 * K\n",
    "        \n",
    "        if r**2 - (Cw**2 / 4) < 0:\n",
    "            return np.nan\n",
    "        \n",
    "        H = r - np.sqrt(r**2 - (Cw**2 / 4))\n",
    "        ACDconst = 0.62467 * A_const - 68.747\n",
    "        offset = ACDconst - 3.336\n",
    "        ACDest = H + offset\n",
    "        \n",
    "        RETHICK = 0.65696 - 0.02029 * AL\n",
    "        LOPT = AL + RETHICK\n",
    "        \n",
    "        ncm1 = nc - 1\n",
    "        IOL = (1000 * na * (na * r - ncm1 * LOPT)) / ((LOPT - ACDest) * (na * r - ncm1 * ACDest))\n",
    "        \n",
    "        return IOL\n",
    "    except:\n",
    "        return np.nan\n",
    "\n",
    "# Calculate baseline performance\n",
    "df_train_val['SRKT_Baseline'] = df_train_val.apply(\n",
    "    lambda row: calculate_SRKT(row['Bio-AL'], row['K_avg_Kerato'], row['A-Constant']), \n",
    "    axis=1\n",
    ")\n",
    "\n",
    "df_train_val['Expected_SE'] = -(df_train_val['IOL Power'] - df_train_val['SRKT_Baseline'])\n",
    "df_train_val['SRKT_Error'] = df_train_val['PostOP Spherical Equivalent'] - df_train_val['Expected_SE']\n",
    "\n",
    "valid_train = df_train_val['SRKT_Error'].notna()\n",
    "baseline_mae = df_train_val.loc[valid_train, 'SRKT_Error'].abs().mean()\n",
    "\n",
    "print(f\"Baseline MAE (training): {baseline_mae:.3f} D\")\n",
    "print(f\"Our target: Beat {baseline_mae - 0.05:.3f} D for significance\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceb1d089",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% Cell 5: Simple Grid Search for nc (NO OPTUNA NEEDED)\n",
    "print(\"NC OPTIMIZATION: Grid Search Approach\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Prepare data\n",
    "df_opt_train = df_train_val[df_train_val['SRKT_Error'].notna()].copy()\n",
    "\n",
    "# Test a range of nc values\n",
    "nc_range = np.arange(1.330, 1.338, 0.0002)\n",
    "print(f\"Testing {len(nc_range)} different nc values...\")\n",
    "\n",
    "cv_scores = []\n",
    "for nc in nc_range:\n",
    "    # Calculate CV score\n",
    "    kf = KFold(n_splits=3, shuffle=True, random_state=RANDOM_SEED)\n",
    "    fold_maes = []\n",
    "    \n",
    "    for train_idx, val_idx in kf.split(df_opt_train):\n",
    "        val_data = df_opt_train.iloc[val_idx]\n",
    "        \n",
    "        predictions = val_data.apply(\n",
    "            lambda row: calculate_SRKT(row['Bio-AL'], row['K_avg_Kerato'], \n",
    "                                     row['A-Constant'], nc=nc), \n",
    "            axis=1\n",
    "        )\n",
    "        \n",
    "        expected_se = -(val_data['IOL Power'] - predictions)\n",
    "        errors = val_data['PostOP Spherical Equivalent'] - expected_se\n",
    "        \n",
    "        valid_errors = errors[errors.notna()]\n",
    "        if len(valid_errors) > 0:\n",
    "            fold_maes.append(valid_errors.abs().mean())\n",
    "    \n",
    "    cv_scores.append(np.mean(fold_maes) if fold_maes else 999)\n",
    "\n",
    "# Find optimal nc\n",
    "cv_scores = np.array(cv_scores)\n",
    "best_idx = np.argmin(cv_scores)\n",
    "optimal_nc = nc_range[best_idx]\n",
    "\n",
    "print(f\"\\nOptimal nc: {optimal_nc:.5f}\")\n",
    "print(f\"Best CV MAE: {cv_scores[best_idx]:.3f} D\")\n",
    "\n",
    "# Visualize\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(nc_range, cv_scores, 'b-', linewidth=2)\n",
    "plt.scatter(optimal_nc, cv_scores[best_idx], s=200, c='red', marker='*')\n",
    "plt.axvline(x=1.333, color='gray', linestyle='--', alpha=0.5, label='Standard nc')\n",
    "plt.xlabel('Corneal Refractive Index (nc)')\n",
    "plt.ylabel('Cross-Validation MAE (D)')\n",
    "plt.title('Grid Search for Optimal nc')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Apply optimal nc\n",
    "df_opt_train['SRKT_ML_nc'] = df_opt_train.apply(\n",
    "    lambda row: calculate_SRKT(row['Bio-AL'], row['K_avg_Kerato'], \n",
    "                              row['A-Constant'], nc=optimal_nc), \n",
    "    axis=1\n",
    ")\n",
    "df_opt_train['Expected_SE_ML_nc'] = -(df_opt_train['IOL Power'] - df_opt_train['SRKT_ML_nc'])\n",
    "df_opt_train['Error_ML_nc'] = df_opt_train['PostOP Spherical Equivalent'] - df_opt_train['Expected_SE_ML_nc']\n",
    "\n",
    "ml_nc_mae = df_opt_train['Error_ML_nc'].abs().mean()\n",
    "print(f\"\\nTraining MAE with optimized nc: {ml_nc_mae:.3f} D\")\n",
    "print(f\"Improvement: {(baseline_mae - ml_nc_mae)/baseline_mae * 100:.1f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f561bd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% Cell 6: Robust Ensemble with Proper Standardization\n",
    "print(\"\\nIMPROVED OPTIMIZATION STRATEGY #2: Robust Ensemble\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Feature engineering for ensemble - INCLUDING CCT without assumptions\n",
    "feature_cols = ['Bio-AL', 'K_avg_Kerato', 'A-Constant', 'K_Cylinder', \n",
    "                'Post_Ant_Ratio', 'AL_K_Ratio', 'CCT']  # Added CCT directly\n",
    "\n",
    "# Add additional features if available\n",
    "if 'Posterior Km' in df_opt_train.columns:\n",
    "    feature_cols.append('Posterior Km')\n",
    "if 'Anterior Km' in df_opt_train.columns:\n",
    "    feature_cols.append('Anterior Km')\n",
    "if 'Bio-ACD' in df_opt_train.columns:\n",
    "    feature_cols.append('Bio-ACD')\n",
    "if 'Bio-LT' in df_opt_train.columns:\n",
    "    feature_cols.append('Bio-LT')\n",
    "\n",
    "print(f\"\\nUsing features: {feature_cols}\")\n",
    "\n",
    "# Handle missing values with KNN imputation\n",
    "imputer = KNNImputer(n_neighbors=5)\n",
    "\n",
    "X_train_imputed = pd.DataFrame(\n",
    "    imputer.fit_transform(df_opt_train[feature_cols]),\n",
    "    columns=feature_cols,\n",
    "    index=df_opt_train.index\n",
    ")\n",
    "\n",
    "# Target: residual errors after ML-nc optimization\n",
    "y_residuals = df_opt_train['Error_ML_nc']\n",
    "\n",
    "# IMPORTANT: All standardization happens INSIDE the pipelines\n",
    "# This ensures proper scaling without data leakage\n",
    "print(\"\\nBuilding ensemble with proper standardization...\")\n",
    "\n",
    "# 1. Ridge Regression with RobustScaler (handles outliers better)\n",
    "ridge_pipeline = Pipeline([\n",
    "    ('scaler', RobustScaler()),  # This will standardize ALL features properly\n",
    "    ('ridge', Ridge(alpha=10.0))\n",
    "])\n",
    "\n",
    "# 2. Huber Regressor (robust to outliers)\n",
    "huber_pipeline = Pipeline([\n",
    "    ('scaler', RobustScaler()),  # Consistent scaling\n",
    "    ('huber', HuberRegressor(epsilon=1.5, alpha=0.01))\n",
    "])\n",
    "\n",
    "# 3. Random Forest (tree-based, less sensitive to scaling but included for consistency)\n",
    "rf_pipeline = Pipeline([\n",
    "    ('scaler', RobustScaler()),  # Even though RF doesn't need scaling, keep consistent\n",
    "    ('rf', RandomForestRegressor(\n",
    "        n_estimators=20,\n",
    "        max_depth=3,\n",
    "        min_samples_split=15,\n",
    "        min_samples_leaf=10,\n",
    "        random_state=RANDOM_SEED  # USE VARIABLE SEED\n",
    "    ))\n",
    "])\n",
    "\n",
    "# 4. ElasticNet with StandardScaler (for comparison)\n",
    "elastic_pipeline = Pipeline([\n",
    "    ('scaler', StandardScaler()),  # Standard scaling (mean=0, std=1)\n",
    "    ('elastic', ElasticNet(alpha=0.1, l1_ratio=0.5, random_state=RANDOM_SEED))  # USE VARIABLE SEED\n",
    "])\n",
    "\n",
    "# Create voting ensemble\n",
    "ensemble = VotingRegressor([\n",
    "    ('ridge', ridge_pipeline),\n",
    "    ('huber', huber_pipeline),\n",
    "    ('rf', rf_pipeline),\n",
    "    ('elastic', elastic_pipeline)\n",
    "])\n",
    "\n",
    "# Cross-validate ensemble\n",
    "cv_scores = cross_val_score(ensemble, X_train_imputed, y_residuals, \n",
    "                           cv=3, scoring='neg_mean_absolute_error')\n",
    "print(f\"Ensemble CV MAE: {-cv_scores.mean():.3f} (±{cv_scores.std():.3f})\")\n",
    "\n",
    "# Fit ensemble on all training data\n",
    "ensemble.fit(X_train_imputed, y_residuals)\n",
    "\n",
    "# Make predictions\n",
    "ensemble_corrections = ensemble.predict(X_train_imputed)\n",
    "\n",
    "# Apply corrections with dampening factor\n",
    "dampening_factor = 0.5\n",
    "df_opt_train['SRKT_Final'] = (\n",
    "    df_opt_train['SRKT_ML_nc'] + \n",
    "    dampening_factor * ensemble_corrections\n",
    ")\n",
    "\n",
    "df_opt_train['Expected_SE_Final'] = -(df_opt_train['IOL Power'] - df_opt_train['SRKT_Final'])\n",
    "df_opt_train['Error_Final'] = df_opt_train['PostOP Spherical Equivalent'] - df_opt_train['Expected_SE_Final']\n",
    "\n",
    "final_mae = df_opt_train['Error_Final'].abs().mean()\n",
    "print(f\"\\nTraining MAE with ensemble: {final_mae:.3f} D\")\n",
    "print(f\"Total improvement: {(baseline_mae - final_mae)/baseline_mae * 100:.1f}%\")\n",
    "\n",
    "# Save model info\n",
    "model_info = {\n",
    "    'optimal_nc': optimal_nc,\n",
    "    'ensemble': ensemble,\n",
    "    'imputer': imputer,\n",
    "    'feature_cols': feature_cols,\n",
    "    'dampening_factor': dampening_factor,\n",
    "    'random_seed': RANDOM_SEED  # Save the seed used\n",
    "}\n",
    "\n",
    "# Show feature statistics AFTER imputation to verify no assumptions\n",
    "print(\"\\nFeature statistics (after imputation):\")\n",
    "print(X_train_imputed.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1979b85a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% Cell 6.5: Optimize ALL SRK/T Formula Constants\n",
    "print(\"\\nOPTIMIZING ALL SRK/T FORMULA CONSTANTS\")\n",
    "print(\"=\"*60)\n",
    "print(\"This will optimize all 9 parameters, not just nc\")\n",
    "print(f\"Using random seed: {RANDOM_SEED}\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Extended SRK/T with learnable parameters\n",
    "def calculate_SRKT_ml(AL, K, A_const, params):\n",
    "    \"\"\"\n",
    "    ML-optimized SRK/T formula with all parameters optimizable\n",
    "    params = [nc, cw_const1, cw_const2, cw_const3, acd_const1, acd_const2, \n",
    "              offset_const, rethick_const1, rethick_const2]\n",
    "    \"\"\"\n",
    "    # Unpack all parameters\n",
    "    nc = params[0]\n",
    "    cw_const1, cw_const2, cw_const3 = params[1:4]\n",
    "    acd_const1, acd_const2 = params[4:6]\n",
    "    offset_const = params[6]\n",
    "    rethick_const1, rethick_const2 = params[7:9]\n",
    "    \n",
    "    if pd.isna(AL) or pd.isna(K) or pd.isna(A_const) or K <= 0 or AL <= 0:\n",
    "        return np.nan\n",
    "    \n",
    "    try:\n",
    "        na = 1.336\n",
    "        r = 337.5 / K\n",
    "        \n",
    "        # Use standard threshold\n",
    "        if AL > 24.2:\n",
    "            LCOR = -3.446 + 1.716 * AL - 0.0237 * AL**2\n",
    "        else:\n",
    "            LCOR = AL\n",
    "        \n",
    "        # Optimized corneal width calculation\n",
    "        Cw = cw_const1 + cw_const2 * LCOR + cw_const3 * K\n",
    "        \n",
    "        if r**2 - (Cw**2 / 4) < 0:\n",
    "            return np.nan\n",
    "        \n",
    "        H = r - np.sqrt(r**2 - (Cw**2 / 4))\n",
    "        \n",
    "        # Optimized ACD calculation\n",
    "        ACDconst = acd_const1 * A_const - acd_const2\n",
    "        offset = ACDconst - offset_const\n",
    "        ACDest = H + offset\n",
    "        \n",
    "        # Optimized retinal thickness\n",
    "        RETHICK = rethick_const1 - rethick_const2 * AL\n",
    "        LOPT = AL + RETHICK\n",
    "        \n",
    "        ncm1 = nc - 1\n",
    "        IOL = (1000 * na * (na * r - ncm1 * LOPT)) / ((LOPT - ACDest) * (na * r - ncm1 * ACDest))\n",
    "        \n",
    "        return IOL\n",
    "    except:\n",
    "        return np.nan\n",
    "\n",
    "# Objective function for optimizing ALL constants\n",
    "def objective_all_constants(params):\n",
    "    \"\"\"Cross-validation based objective with strong regularization\"\"\"\n",
    "    errors_all = []\n",
    "    \n",
    "    # 3-fold cross-validation\n",
    "    kf = KFold(n_splits=3, shuffle=True, random_state=RANDOM_SEED)  # USE VARIABLE SEED\n",
    "    \n",
    "    for train_idx, val_idx in kf.split(df_opt_train):\n",
    "        val_data = df_opt_train.iloc[val_idx]\n",
    "        \n",
    "        predictions = val_data.apply(\n",
    "            lambda row: calculate_SRKT_ml(row['Bio-AL'], row['K_avg_Kerato'], \n",
    "                                         row['A-Constant'], params), \n",
    "            axis=1\n",
    "        )\n",
    "        \n",
    "        expected_se = -(val_data['IOL Power'] - predictions)\n",
    "        errors = val_data['PostOP Spherical Equivalent'] - expected_se\n",
    "        \n",
    "        valid_errors = errors[errors.notna()]\n",
    "        if len(valid_errors) > 0:\n",
    "            errors_all.extend(valid_errors.abs().tolist())\n",
    "    \n",
    "    if len(errors_all) == 0:\n",
    "        return 999\n",
    "    \n",
    "    mae = np.mean(errors_all)\n",
    "    \n",
    "    # Add regularization to prevent extreme values\n",
    "    # Penalize deviation from standard values\n",
    "    standard_params = [1.333, -5.41, 0.58412, 0.098, 0.62467, 68.747, 3.336, 0.65696, 0.02029]\n",
    "    penalty = 0\n",
    "    for i, (opt, std) in enumerate(zip(params, standard_params)):\n",
    "        # Different weights for different parameters\n",
    "        if i == 0:  # nc - allow more flexibility\n",
    "            penalty += 5 * ((opt - std) / std) ** 2\n",
    "        else:  # other parameters - stronger regularization\n",
    "            penalty += 20 * ((opt - std) / std) ** 2\n",
    "    \n",
    "    return mae + penalty\n",
    "\n",
    "# Initial parameters (standard SRK/T values)\n",
    "initial_params = [\n",
    "    1.333,          # nc\n",
    "    -5.41,          # cw_const1\n",
    "    0.58412,        # cw_const2\n",
    "    0.098,          # cw_const3\n",
    "    0.62467,        # acd_const1\n",
    "    68.747,         # acd_const2\n",
    "    3.336,          # offset_const\n",
    "    0.65696,        # rethick_const1\n",
    "    0.02029         # rethick_const2\n",
    "]\n",
    "\n",
    "# Bounds for optimization (tighter bounds for stability)\n",
    "bounds = [\n",
    "    (1.330, 1.340),           # nc\n",
    "    (-5.8, -5.0),             # cw_const1\n",
    "    (0.55, 0.62),             # cw_const2\n",
    "    (0.090, 0.106),           # cw_const3\n",
    "    (0.60, 0.65),             # acd_const1\n",
    "    (65.0, 72.0),             # acd_const2\n",
    "    (3.1, 3.6),               # offset_const\n",
    "    (0.63, 0.68),             # rethick_const1\n",
    "    (0.018, 0.023)            # rethick_const2\n",
    "]\n",
    "\n",
    "print(\"\\nOptimizing SRK/T formula constants using Differential Evolution...\")\n",
    "print(\"This may take a few minutes...\")\n",
    "\n",
    "# Run optimization with more conservative settings\n",
    "result_de = differential_evolution(\n",
    "    objective_all_constants, \n",
    "    bounds,\n",
    "    maxiter=50,     # More iterations\n",
    "    popsize=15,\n",
    "    seed=RANDOM_SEED,  # USE VARIABLE SEED\n",
    "    atol=0.0001,\n",
    "    tol=0.0001,\n",
    "    disp=True,\n",
    "    workers=1,\n",
    "    strategy='best1bin',\n",
    "    mutation=(0.5, 1),  # Conservative mutation\n",
    "    recombination=0.7\n",
    ")\n",
    "\n",
    "optimal_params_all = result_de.x\n",
    "\n",
    "print(f\"\\nOptimization complete!\")\n",
    "print(f\"Final objective value: {result_de.fun:.4f}\")\n",
    "\n",
    "# Display optimized parameters\n",
    "param_names = ['nc', 'cw_const1', 'cw_const2', 'cw_const3', 'acd_const1', \n",
    "               'acd_const2', 'offset_const', 'rethick_const1', 'rethick_const2']\n",
    "\n",
    "print(\"\\nOptimized Parameters:\")\n",
    "print(\"-\" * 60)\n",
    "for name, original, optimized in zip(param_names, initial_params, optimal_params_all):\n",
    "    change_pct = (optimized - original) / abs(original) * 100\n",
    "    print(f\"{name:20s}: {original:8.5f} → {optimized:8.5f} ({change_pct:+6.2f}%)\")\n",
    "\n",
    "# Calculate performance with fully optimized formula\n",
    "df_opt_train['SRKT_ML_Full'] = df_opt_train.apply(\n",
    "    lambda row: calculate_SRKT_ml(row['Bio-AL'], row['K_avg_Kerato'], \n",
    "                                 row['A-Constant'], optimal_params_all), \n",
    "    axis=1\n",
    ")\n",
    "\n",
    "df_opt_train['Expected_SE_ML_Full'] = -(df_opt_train['IOL Power'] - df_opt_train['SRKT_ML_Full'])\n",
    "df_opt_train['Error_ML_Full'] = df_opt_train['PostOP Spherical Equivalent'] - df_opt_train['Expected_SE_ML_Full']\n",
    "\n",
    "# Compare performances\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"TRAINING SET PERFORMANCE COMPARISON:\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "baseline_train_mae = df_opt_train['SRKT_Error'].abs().mean()\n",
    "nc_only_mae = df_opt_train['Error_ML_nc'].abs().mean()\n",
    "full_opt_mae = df_opt_train['Error_ML_Full'].abs().mean()\n",
    "\n",
    "print(f\"Baseline SRK/T:           MAE = {baseline_train_mae:.3f} D\")\n",
    "print(f\"Optimized nc only:        MAE = {nc_only_mae:.3f} D ({(baseline_train_mae-nc_only_mae)/baseline_train_mae*100:.1f}% improvement)\")\n",
    "print(f\"Fully optimized SRK/T:    MAE = {full_opt_mae:.3f} D ({(baseline_train_mae-full_opt_mae)/baseline_train_mae*100:.1f}% improvement)\")\n",
    "\n",
    "# Update model_info to include full optimization\n",
    "model_info['optimal_params_all'] = optimal_params_all\n",
    "model_info['param_names'] = param_names\n",
    "model_info['calculate_SRKT_ml'] = calculate_SRKT_ml\n",
    "\n",
    "print(\"\\nFull formula optimization complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2fd32f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% Cell 7: Final Test Set Evaluation with Bootstrap Confidence Intervals\n",
    "print(\"\\n\" + \"#\"*80)\n",
    "print(\"#\" + \" \"*30 + \"FINAL TEST SET EVALUATION\" + \" \"*24 + \"#\")\n",
    "print(\"#\"*80)\n",
    "print(\"\\nUsing bootstrap for more reliable significance testing\")\n",
    "print(f\"Random seed: {RANDOM_SEED}\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Prepare test data\n",
    "df_test_eval = df_test.copy()\n",
    "\n",
    "# 1. Baseline SRK/T\n",
    "print(\"\\nCalculating baseline SRK/T...\")\n",
    "df_test_eval['SRKT_Baseline'] = df_test_eval.apply(\n",
    "    lambda row: calculate_SRKT(row['Bio-AL'], row['K_avg_Kerato'], \n",
    "                              row['A-Constant'], nc=1.333), \n",
    "    axis=1\n",
    ")\n",
    "df_test_eval['Expected_SE_Baseline'] = -(df_test_eval['IOL Power'] - df_test_eval['SRKT_Baseline'])\n",
    "df_test_eval['Error_Baseline'] = df_test_eval['PostOP Spherical Equivalent'] - df_test_eval['Expected_SE_Baseline']\n",
    "\n",
    "# 2. ML-optimized nc only\n",
    "print(\"Calculating ML-optimized nc...\")\n",
    "df_test_eval['SRKT_ML_nc'] = df_test_eval.apply(\n",
    "    lambda row: calculate_SRKT(row['Bio-AL'], row['K_avg_Kerato'], \n",
    "                              row['A-Constant'], nc=model_info['optimal_nc']), \n",
    "    axis=1\n",
    ")\n",
    "df_test_eval['Expected_SE_ML_nc'] = -(df_test_eval['IOL Power'] - df_test_eval['SRKT_ML_nc'])\n",
    "df_test_eval['Error_ML_nc'] = df_test_eval['PostOP Spherical Equivalent'] - df_test_eval['Expected_SE_ML_nc']\n",
    "\n",
    "# 3. Fully optimized SRK/T (all constants)\n",
    "print(\"Calculating fully optimized SRK/T...\")\n",
    "if 'optimal_params_all' in model_info:\n",
    "    df_test_eval['SRKT_ML_Full'] = df_test_eval.apply(\n",
    "        lambda row: calculate_SRKT_ml(row['Bio-AL'], row['K_avg_Kerato'], \n",
    "                                     row['A-Constant'], model_info['optimal_params_all']), \n",
    "        axis=1\n",
    "    )\n",
    "    df_test_eval['Expected_SE_ML_Full'] = -(df_test_eval['IOL Power'] - df_test_eval['SRKT_ML_Full'])\n",
    "    df_test_eval['Error_ML_Full'] = df_test_eval['PostOP Spherical Equivalent'] - df_test_eval['Expected_SE_ML_Full']\n",
    "\n",
    "# 4. ML + Ensemble\n",
    "print(\"Calculating ensemble predictions...\")\n",
    "# Prepare features using the same imputer from training\n",
    "X_test = df_test_eval[model_info['feature_cols']].copy()\n",
    "\n",
    "# Use the fitted imputer from training\n",
    "X_test_imputed = pd.DataFrame(\n",
    "    model_info['imputer'].transform(X_test),\n",
    "    columns=model_info['feature_cols'],\n",
    "    index=X_test.index\n",
    ")\n",
    "\n",
    "# Make ensemble predictions\n",
    "ensemble_corrections = model_info['ensemble'].predict(X_test_imputed)\n",
    "df_test_eval['SRKT_Final'] = (\n",
    "    df_test_eval['SRKT_ML_nc'] + \n",
    "    model_info['dampening_factor'] * ensemble_corrections\n",
    ")\n",
    "df_test_eval['Expected_SE_Final'] = -(df_test_eval['IOL Power'] - df_test_eval['SRKT_Final'])\n",
    "df_test_eval['Error_Final'] = df_test_eval['PostOP Spherical Equivalent'] - df_test_eval['Expected_SE_Final']\n",
    "\n",
    "# Calculate metrics\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"TEST SET RESULTS:\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "methods = {\n",
    "    'Baseline SRK/T': df_test_eval['Error_Baseline'].abs(),\n",
    "    'ML-Optimized nc': df_test_eval['Error_ML_nc'].abs(),\n",
    "}\n",
    "\n",
    "# Add fully optimized if available\n",
    "if 'Error_ML_Full' in df_test_eval.columns:\n",
    "    methods['ML-Optimized Full'] = df_test_eval['Error_ML_Full'].abs()\n",
    "\n",
    "methods['ML + Ensemble'] = df_test_eval['Error_Final'].abs()\n",
    "\n",
    "# Results summary\n",
    "results_data = []\n",
    "for method, errors in methods.items():\n",
    "    valid_errors = errors.dropna()\n",
    "    mae = valid_errors.mean()\n",
    "    std = valid_errors.std()\n",
    "    within_025 = (valid_errors <= 0.25).sum() / len(valid_errors) * 100\n",
    "    within_050 = (valid_errors <= 0.50).sum() / len(valid_errors) * 100\n",
    "    within_100 = (valid_errors <= 1.00).sum() / len(valid_errors) * 100\n",
    "    \n",
    "    results_data.append({\n",
    "        'Method': method,\n",
    "        'MAE (D)': mae,\n",
    "        'SD (D)': std,\n",
    "        '±0.25D (%)': within_025,\n",
    "        '±0.50D (%)': within_050,\n",
    "        '±1.00D (%)': within_100\n",
    "    })\n",
    "    \n",
    "results_df = pd.DataFrame(results_data)\n",
    "print(results_df.to_string(index=False, float_format=lambda x: f'{x:.3f}' if x < 10 else f'{x:.1f}'))\n",
    "\n",
    "# Bootstrap significance testing with CORRECTED function\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"BOOTSTRAP SIGNIFICANCE TESTING (1000 iterations)\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "def bootstrap_paired_test(errors1, errors2, n_bootstrap=1000):\n",
    "    \"\"\"Bootstrap test for paired samples - RETURNS bootstrap_diffs\"\"\"\n",
    "    # Align indices\n",
    "    common_idx = errors1.index.intersection(errors2.index)\n",
    "    e1 = errors1[common_idx].values\n",
    "    e2 = errors2[common_idx].values\n",
    "    \n",
    "    # Calculate observed difference\n",
    "    observed_diff = np.mean(e1) - np.mean(e2)\n",
    "    \n",
    "    # Bootstrap\n",
    "    bootstrap_diffs = []\n",
    "    n_samples = len(e1)\n",
    "    \n",
    "    # Set seed for bootstrap\n",
    "    np.random.seed(RANDOM_SEED)\n",
    "    \n",
    "    for _ in range(n_bootstrap):\n",
    "        # Resample with replacement\n",
    "        idx = np.random.choice(n_samples, n_samples, replace=True)\n",
    "        boot_diff = np.mean(e1[idx]) - np.mean(e2[idx])\n",
    "        bootstrap_diffs.append(boot_diff)\n",
    "    \n",
    "    bootstrap_diffs = np.array(bootstrap_diffs)\n",
    "    \n",
    "    # Calculate p-value (two-sided)\n",
    "    p_value = np.sum(bootstrap_diffs <= 0) / n_bootstrap * 2\n",
    "    p_value = min(p_value, 1.0)\n",
    "    \n",
    "    # Confidence interval\n",
    "    ci_lower = np.percentile(bootstrap_diffs, 2.5)\n",
    "    ci_upper = np.percentile(bootstrap_diffs, 97.5)\n",
    "    \n",
    "    return observed_diff, p_value, ci_lower, ci_upper, bootstrap_diffs\n",
    "\n",
    "# Test ML methods against baseline\n",
    "baseline_errors = df_test_eval['Error_Baseline'].abs().dropna()\n",
    "\n",
    "# 1. ML-nc vs Baseline\n",
    "print(\"\\n1. ML-Optimized nc vs Baseline:\")\n",
    "ml_nc_errors = df_test_eval['Error_ML_nc'].abs().dropna()\n",
    "diff_nc, p_nc, ci_lower_nc, ci_upper_nc, bootstrap_diffs_nc = bootstrap_paired_test(\n",
    "    baseline_errors, ml_nc_errors\n",
    ")\n",
    "print(f\"   Mean difference: {diff_nc:.4f} D\")\n",
    "print(f\"   95% CI: [{ci_lower_nc:.4f}, {ci_upper_nc:.4f}]\")\n",
    "print(f\"   Bootstrap p-value: {p_nc:.4f}\")\n",
    "print(f\"   Significant at α=0.05: {'Yes' if p_nc < 0.05 else 'No'}\")\n",
    "\n",
    "# 2. Fully optimized vs Baseline (if available)\n",
    "if 'Error_ML_Full' in df_test_eval.columns:\n",
    "    print(\"\\n2. Fully Optimized SRK/T vs Baseline:\")\n",
    "    ml_full_errors = df_test_eval['Error_ML_Full'].abs().dropna()\n",
    "    diff_full, p_full, ci_lower_full, ci_upper_full, bootstrap_diffs_full = bootstrap_paired_test(\n",
    "        baseline_errors, ml_full_errors\n",
    "    )\n",
    "    print(f\"   Mean difference: {diff_full:.4f} D\")\n",
    "    print(f\"   95% CI: [{ci_lower_full:.4f}, {ci_upper_full:.4f}]\")\n",
    "    print(f\"   Bootstrap p-value: {p_full:.4f}\")\n",
    "    print(f\"   Significant at α=0.05: {'Yes' if p_full < 0.05 else 'No'}\")\n",
    "\n",
    "# 3. ML+Ensemble vs Baseline\n",
    "print(\"\\n3. ML + Ensemble vs Baseline:\")\n",
    "final_errors = df_test_eval['Error_Final'].abs().dropna()\n",
    "diff_final, p_final, ci_lower_final, ci_upper_final, bootstrap_diffs = bootstrap_paired_test(\n",
    "    baseline_errors, final_errors\n",
    ")\n",
    "print(f\"   Mean difference: {diff_final:.4f} D\")\n",
    "print(f\"   95% CI: [{ci_lower_final:.4f}, {ci_upper_final:.4f}]\")\n",
    "print(f\"   Bootstrap p-value: {p_final:.4f}\")\n",
    "print(f\"   Significant at α=0.05: {'Yes' if p_final < 0.05 else 'No'}\")\n",
    "\n",
    "# Additional paired t-test for comparison\n",
    "from scipy.stats import ttest_rel, wilcoxon\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"ADDITIONAL STATISTICAL TESTS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Paired t-test\n",
    "common_idx = baseline_errors.index.intersection(final_errors.index)\n",
    "if len(common_idx) > 0:\n",
    "    t_stat, p_ttest = ttest_rel(baseline_errors[common_idx], final_errors[common_idx])\n",
    "    print(f\"\\nPaired t-test (ML + Ensemble vs Baseline):\")\n",
    "    print(f\"   p-value: {p_ttest:.4f}\")\n",
    "    print(f\"   Significant at α=0.05: {'Yes' if p_ttest < 0.05 else 'No'}\")\n",
    "    \n",
    "    # Wilcoxon signed-rank test\n",
    "    w_stat, p_wilcoxon = wilcoxon(baseline_errors[common_idx], final_errors[common_idx])\n",
    "    print(f\"\\nWilcoxon signed-rank test (ML + Ensemble vs Baseline):\")\n",
    "    print(f\"   p-value: {p_wilcoxon:.4f}\")\n",
    "    print(f\"   Significant at α=0.05: {'Yes' if p_wilcoxon < 0.05 else 'No'}\")\n",
    "\n",
    "# Visualization\n",
    "fig = plt.figure(figsize=(18, 10))\n",
    "\n",
    "# Plot 1: Error distributions\n",
    "ax1 = plt.subplot(2, 3, 1)\n",
    "data_to_plot = []\n",
    "labels = []\n",
    "for method, errors in methods.items():\n",
    "    if not errors.isna().all():\n",
    "        data_to_plot.append(errors.dropna())\n",
    "        labels.append(method)\n",
    "\n",
    "bp = ax1.boxplot(data_to_plot, labels=labels, patch_artist=True)\n",
    "colors = ['lightblue', 'lightgreen', 'lightcoral', 'gold']\n",
    "for patch, color in zip(bp['boxes'], colors[:len(bp['boxes'])]):\n",
    "    patch.set_facecolor(color)\n",
    "ax1.set_ylabel('Absolute Error (D)')\n",
    "ax1.set_title('Test Set Error Distributions')\n",
    "ax1.tick_params(axis='x', rotation=45)\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 2: Bootstrap distribution (final model)\n",
    "ax2 = plt.subplot(2, 3, 2)\n",
    "ax2.hist(bootstrap_diffs, bins=30, alpha=0.7, edgecolor='black')\n",
    "ax2.axvline(x=0, color='red', linestyle='--', label='No difference')\n",
    "ax2.axvline(x=diff_final, color='green', linestyle='-', linewidth=2, \n",
    "            label=f'Observed diff: {diff_final:.3f}')\n",
    "ax2.set_xlabel('MAE Difference (Baseline - ML+Ensemble)')\n",
    "ax2.set_ylabel('Frequency')\n",
    "ax2.set_title('Bootstrap Distribution')\n",
    "ax2.legend()\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 3: Performance comparison bar chart\n",
    "ax3 = plt.subplot(2, 3, 3)\n",
    "x_pos = np.arange(len(results_df))\n",
    "ax3.bar(x_pos, results_df['MAE (D)'], color=colors[:len(results_df)])\n",
    "ax3.set_xticks(x_pos)\n",
    "ax3.set_xticklabels(results_df['Method'], rotation=45, ha='right')\n",
    "ax3.set_ylabel('MAE (D)')\n",
    "ax3.set_title('Mean Absolute Error Comparison')\n",
    "ax3.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# Plot 4: Percentage within target\n",
    "ax4 = plt.subplot(2, 3, 4)\n",
    "width = 0.25\n",
    "x_pos = np.arange(len(results_df))\n",
    "ax4.bar(x_pos - width, results_df['±0.25D (%)'], width, label='±0.25D', color='darkgreen')\n",
    "ax4.bar(x_pos, results_df['±0.50D (%)'], width, label='±0.50D', color='orange')\n",
    "ax4.bar(x_pos + width, results_df['±1.00D (%)'], width, label='±1.00D', color='blue')\n",
    "ax4.set_xticks(x_pos)\n",
    "ax4.set_xticklabels(results_df['Method'], rotation=45, ha='right')\n",
    "ax4.set_ylabel('Percentage (%)')\n",
    "ax4.set_title('Percentage Within Target Range')\n",
    "ax4.legend()\n",
    "ax4.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# Plot 5: Scatter plot of best method\n",
    "ax5 = plt.subplot(2, 3, 5)\n",
    "valid_final = df_test_eval['Expected_SE_Final'].notna()\n",
    "if valid_final.sum() > 0:\n",
    "    ax5.scatter(df_test_eval.loc[valid_final, 'Expected_SE_Final'], \n",
    "               df_test_eval.loc[valid_final, 'PostOP Spherical Equivalent'], \n",
    "               alpha=0.6, color='gold')\n",
    "    \n",
    "    # Add perfect prediction line\n",
    "    min_val = df_test_eval.loc[valid_final, 'Expected_SE_Final'].min()\n",
    "    max_val = df_test_eval.loc[valid_final, 'Expected_SE_Final'].max()\n",
    "    ax5.plot([min_val, max_val], [min_val, max_val], 'r--', label='Perfect Prediction')\n",
    "    ax5.set_xlabel('Predicted SE (D)')\n",
    "    ax5.set_ylabel('Actual SE (D)')\n",
    "    ax5.set_title('ML + Ensemble: Predicted vs Actual')\n",
    "    ax5.legend()\n",
    "    ax5.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 6: Improvement summary\n",
    "ax6 = plt.subplot(2, 3, 6)\n",
    "baseline_mae = results_df.loc[results_df['Method'] == 'Baseline SRK/T', 'MAE (D)'].values[0]\n",
    "improvements = []\n",
    "for _, row in results_df.iterrows():\n",
    "    improvement = (baseline_mae - row['MAE (D)']) / baseline_mae * 100\n",
    "    improvements.append(improvement)\n",
    "\n",
    "bars = ax6.bar(range(len(results_df)), improvements, color=colors[:len(results_df)])\n",
    "ax6.set_ylabel('Improvement over Baseline (%)')\n",
    "ax6.set_title('Relative Performance Improvement')\n",
    "ax6.set_xticks(range(len(results_df)))\n",
    "ax6.set_xticklabels(results_df['Method'], rotation=45, ha='right')\n",
    "ax6.axhline(y=0, color='black', linestyle='-', linewidth=0.5)\n",
    "ax6.grid(True, alpha=0.3)\n",
    "\n",
    "# Add value labels on bars\n",
    "for bar, imp in zip(bars, improvements):\n",
    "    height = bar.get_height()\n",
    "    ax6.annotate(f'{imp:.1f}%',\n",
    "                xy=(bar.get_x() + bar.get_width() / 2, height),\n",
    "                xytext=(0, 3 if height > 0 else -15),\n",
    "                textcoords=\"offset points\",\n",
    "                ha='center', va='bottom' if height > 0 else 'top')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"ANALYSIS COMPLETE!\")\n",
    "print(\"\\nKey findings:\")\n",
    "print(f\"1. Test set size: {len(df_test)} eyes (30% of total)\")\n",
    "print(f\"2. Best performing method: {results_df.loc[results_df['MAE (D)'].idxmin(), 'Method']}\")\n",
    "print(f\"3. Best MAE: {results_df['MAE (D)'].min():.3f} D\")\n",
    "print(f\"4. Improvement over baseline: {improvements[results_df['MAE (D)'].idxmin()]:.1f}%\")\n",
    "print(f\"5. Random seed used: {RANDOM_SEED}\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "542dcb27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% Cell 8: Document the Optimized SRK/T Formula for FacoDMEK Eyes\n",
    "print(\"=\" * 80)\n",
    "print(\"OPTIMIZED SRK/T FORMULA FOR FacoDMEK EYES\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"\\nBased on machine learning optimization of {len(df_complete)} FacoDMEK cases\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Section 1: NC-ONLY OPTIMIZATION\n",
    "print(\"\\n1. NC-ONLY OPTIMIZATION:\")\n",
    "print(\"-\" * 60)\n",
    "print(f\"Original corneal refractive index (nc): 1.3330\")\n",
    "print(f\"Optimized corneal refractive index (nc): {model_info['optimal_nc']:.5f}\")\n",
    "print(f\"Change: {(model_info['optimal_nc'] - 1.333):.5f} ({((model_info['optimal_nc'] - 1.333)/1.333 * 100):.2f}%)\")\n",
    "\n",
    "# Section 2: FULL PARAMETER OPTIMIZATION\n",
    "if 'optimal_params_all' in model_info:\n",
    "    print(\"\\n2. FULL PARAMETER OPTIMIZATION:\")\n",
    "    print(\"-\" * 60)\n",
    "    print(\"All optimized SRK/T parameters:\")\n",
    "    print()\n",
    "    \n",
    "    standard_params = [1.333, -5.41, 0.58412, 0.098, 0.62467, 68.747, 3.336, 0.65696, 0.02029]\n",
    "    param_names = ['nc', 'cw_const1', 'cw_const2', 'cw_const3', 'acd_const1', \n",
    "                   'acd_const2', 'offset_const', 'rethick_const1', 'rethick_const2']\n",
    "    \n",
    "    # Create comparison table\n",
    "    comparison_data = []\n",
    "    for i, name in enumerate(param_names):\n",
    "        original = standard_params[i]\n",
    "        optimized = model_info['optimal_params_all'][i]\n",
    "        change_pct = (optimized - original) / abs(original) * 100\n",
    "        comparison_data.append({\n",
    "            'Parameter': name,\n",
    "            'Standard': original,\n",
    "            'Optimized': optimized,\n",
    "            'Change (%)': change_pct\n",
    "        })\n",
    "    \n",
    "    param_df = pd.DataFrame(comparison_data)\n",
    "    print(param_df.to_string(index=False, float_format=lambda x: f'{x:.5f}' if abs(x) < 100 else f'{x:.2f}'))\n",
    "\n",
    "# Section 3: PERFORMANCE COMPARISON\n",
    "print(\"\\n3. PERFORMANCE COMPARISON (Test Set):\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "if 'df_test_eval' in globals():\n",
    "    baseline_mae = df_test_eval['Error_Baseline'].abs().dropna().mean()\n",
    "    nc_mae = df_test_eval['Error_ML_nc'].abs().dropna().mean()\n",
    "    \n",
    "    print(f\"Standard SRK/T (nc=1.3330):\")\n",
    "    print(f\"  MAE: {baseline_mae:.3f} D\")\n",
    "    \n",
    "    print(f\"\\nOptimized nc only (nc={model_info['optimal_nc']:.5f}):\")\n",
    "    print(f\"  MAE: {nc_mae:.3f} D\")\n",
    "    print(f\"  Improvement: {(baseline_mae - nc_mae)/baseline_mae * 100:.1f}%\")\n",
    "    \n",
    "    if 'Error_ML_Full' in df_test_eval.columns:\n",
    "        full_mae = df_test_eval['Error_ML_Full'].abs().dropna().mean()\n",
    "        print(f\"\\nFully optimized SRK/T (all parameters):\")\n",
    "        print(f\"  MAE: {full_mae:.3f} D\")\n",
    "        print(f\"  Improvement: {(baseline_mae - full_mae)/baseline_mae * 100:.1f}%\")\n",
    "    \n",
    "    if 'Error_Final' in df_test_eval.columns:\n",
    "        ensemble_mae = df_test_eval['Error_Final'].abs().dropna().mean()\n",
    "        print(f\"\\nML + Ensemble:\")\n",
    "        print(f\"  MAE: {ensemble_mae:.3f} D\")\n",
    "        print(f\"  Improvement: {(baseline_mae - ensemble_mae)/baseline_mae * 100:.1f}%\")\n",
    "\n",
    "# Section 4: FORMULA IMPLEMENTATION\n",
    "print(\"\\n4. FORMULA IMPLEMENTATION:\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "# For nc-only optimization\n",
    "print(\"\\nA. Simple Implementation (nc-only optimization):\")\n",
    "print(\"   Just change nc from 1.333 to {:.5f} in standard SRK/T\".format(model_info['optimal_nc']))\n",
    "\n",
    "# For full optimization\n",
    "if 'optimal_params_all' in model_info:\n",
    "    print(\"\\nB. Full Implementation (all parameters optimized):\")\n",
    "    print(\"\"\"\n",
    "def calculate_SRKT_FacoDMEK_Full(AL, K, A_const):\n",
    "    '''\n",
    "    Fully optimized SRK/T formula for FacoDMEK eyes\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    AL : float - Axial Length in mm\n",
    "    K : float - Average keratometry in diopters\n",
    "    A_const : float - A-constant for the IOL\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    float - Predicted IOL power for emmetropia\n",
    "    '''\n",
    "    # Optimized constants\n",
    "    nc = {:.5f}         # corneal refractive index\n",
    "    cw_const1 = {:.5f}  # corneal width constant 1\n",
    "    cw_const2 = {:.5f}  # corneal width constant 2\n",
    "    cw_const3 = {:.5f}  # corneal width constant 3\n",
    "    acd_const1 = {:.5f} # ACD constant 1\n",
    "    acd_const2 = {:.5f} # ACD constant 2\n",
    "    offset_const = {:.5f} # offset constant\n",
    "    rethick_const1 = {:.5f} # retinal thickness constant 1\n",
    "    rethick_const2 = {:.5f} # retinal thickness constant 2\n",
    "    \n",
    "    # Standard constants (unchanged)\n",
    "    na = 1.336  # aqueous/vitreous refractive index\n",
    "    \n",
    "    # Corneal radius\n",
    "    r = 337.5 / K\n",
    "    \n",
    "    # Axial length correction\n",
    "    if AL > 24.2:\n",
    "        LCOR = -3.446 + 1.716 * AL - 0.0237 * AL**2\n",
    "    else:\n",
    "        LCOR = AL\n",
    "    \n",
    "    # Corneal width\n",
    "    Cw = cw_const1 + cw_const2 * LCOR + cw_const3 * K\n",
    "    \n",
    "    # Corneal height\n",
    "    H = r - sqrt(r**2 - (Cw**2 / 4))\n",
    "    \n",
    "    # ACD calculation\n",
    "    ACDconst = acd_const1 * A_const - acd_const2\n",
    "    offset = ACDconst - offset_const\n",
    "    ACDest = H + offset\n",
    "    \n",
    "    # Retinal thickness\n",
    "    RETHICK = rethick_const1 - rethick_const2 * AL\n",
    "    LOPT = AL + RETHICK\n",
    "    \n",
    "    # IOL power calculation\n",
    "    ncm1 = nc - 1\n",
    "    IOL = (1000 * na * (na * r - ncm1 * LOPT)) / \n",
    "          ((LOPT - ACDest) * (na * r - ncm1 * ACDest))\n",
    "    \n",
    "    return IOL\n",
    "\"\"\".format(*model_info['optimal_params_all']))\n",
    "\n",
    "# Section 5: CLINICAL RECOMMENDATIONS\n",
    "print(\"\\n5. CLINICAL RECOMMENDATIONS:\")\n",
    "print(\"-\" * 60)\n",
    "print(\"\"\"\n",
    "1. For quick implementation: Use nc-only optimization\n",
    "   - Simply change nc from 1.333 to {:.5f}\n",
    "   - Provides {:.1f}% improvement\n",
    "\n",
    "2. For maximum accuracy: Use fully optimized parameters\n",
    "   - Implement all parameter changes\n",
    "   - Provides {:.1f}% improvement\n",
    "\n",
    "3. For research settings: Consider ensemble approach\n",
    "   - Combines optimized formula with ML corrections\n",
    "   - Provides {:.1f}% improvement\n",
    "\n",
    "4. Important notes:\n",
    "   - Use KERATOMETRY values (not biometry K)\n",
    "   - Optimization specific to FacoDMEK eyes\n",
    "   - Based on {} training cases\n",
    "   - Validated on {} test cases\n",
    "\"\"\".format(\n",
    "    model_info['optimal_nc'],\n",
    "    (baseline_mae - nc_mae)/baseline_mae * 100 if 'nc_mae' in locals() else 0,\n",
    "    (baseline_mae - full_mae)/baseline_mae * 100 if 'full_mae' in locals() else 0,\n",
    "    (baseline_mae - ensemble_mae)/baseline_mae * 100 if 'ensemble_mae' in locals() else 0,\n",
    "    len(df_train_val),\n",
    "    len(df_test)\n",
    "))\n",
    "\n",
    "# Section 6: EXAMPLE CALCULATIONS\n",
    "print(\"\\n6. EXAMPLE CALCULATIONS:\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "# Example case\n",
    "AL_ex = 23.5\n",
    "K_ex = 44.0\n",
    "A_const_ex = 118.7\n",
    "\n",
    "print(f\"Example case: AL={AL_ex}mm, K={K_ex}D, A-constant={A_const_ex}\")\n",
    "print()\n",
    "\n",
    "# Calculate with different methods\n",
    "iol_standard = calculate_SRKT(AL_ex, K_ex, A_const_ex, nc=1.333)\n",
    "iol_nc_opt = calculate_SRKT(AL_ex, K_ex, A_const_ex, nc=model_info['optimal_nc'])\n",
    "\n",
    "print(f\"Standard SRK/T:        {iol_standard:.2f} D\")\n",
    "print(f\"Optimized nc only:     {iol_nc_opt:.2f} D (diff: {iol_nc_opt - iol_standard:+.2f} D)\")\n",
    "\n",
    "if 'optimal_params_all' in model_info:\n",
    "    iol_full_opt = calculate_SRKT_ml(AL_ex, K_ex, A_const_ex, model_info['optimal_params_all'])\n",
    "    print(f\"Fully optimized:       {iol_full_opt:.2f} D (diff: {iol_full_opt - iol_standard:+.2f} D)\")\n",
    "\n",
    "# Section 7: SAVE FORMULA DETAILS\n",
    "print(\"\\n7. EXPORT FORMULA:\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "# Create export dictionary\n",
    "export_data = {\n",
    "    'formula_name': 'SRK/T-FacoDMEK-Optimized',\n",
    "    'optimization_date': pd.Timestamp.now().strftime('%Y-%m-%d'),\n",
    "    'training_cases': len(df_train_val),\n",
    "    'test_cases': len(df_test),\n",
    "    'optimal_nc': float(model_info['optimal_nc']),\n",
    "    'baseline_mae': float(baseline_mae) if 'baseline_mae' in locals() else None,\n",
    "    'optimized_nc_mae': float(nc_mae) if 'nc_mae' in locals() else None,\n",
    "}\n",
    "\n",
    "if 'optimal_params_all' in model_info:\n",
    "    export_data['optimal_params_all'] = [float(p) for p in model_info['optimal_params_all']]\n",
    "    export_data['param_names'] = param_names\n",
    "    export_data['full_optimized_mae'] = float(full_mae) if 'full_mae' in locals() else None\n",
    "\n",
    "# Save as JSON\n",
    "import json\n",
    "with open('SRKT_FacoDMEK_optimized_formula.json', 'w') as f:\n",
    "    json.dump(export_data, f, indent=2)\n",
    "\n",
    "# Save human-readable version\n",
    "with open('SRKT_FacoDMEK_optimized_formula.txt', 'w') as f:\n",
    "    f.write(\"OPTIMIZED SRK/T FORMULA FOR FacoDMEK EYES\\n\")\n",
    "    f.write(\"=\"*60 + \"\\n\\n\")\n",
    "    f.write(f\"Date: {export_data['optimization_date']}\\n\")\n",
    "    f.write(f\"Training cases: {export_data['training_cases']}\\n\")\n",
    "    f.write(f\"Test cases: {export_data['test_cases']}\\n\\n\")\n",
    "    \n",
    "    f.write(\"NC-ONLY OPTIMIZATION:\\n\")\n",
    "    f.write(f\"  nc = {export_data['optimal_nc']:.5f} (standard: 1.33300)\\n\\n\")\n",
    "    \n",
    "    if 'optimal_params_all' in model_info:\n",
    "        f.write(\"FULL PARAMETER OPTIMIZATION:\\n\")\n",
    "        for i, name in enumerate(param_names):\n",
    "            f.write(f\"  {name}: {model_info['optimal_params_all'][i]:.5f}\\n\")\n",
    "    \n",
    "    f.write(f\"\\nPERFORMANCE:\\n\")\n",
    "    f.write(f\"  Baseline MAE: {baseline_mae:.3f} D\\n\")\n",
    "    f.write(f\"  NC-optimized MAE: {nc_mae:.3f} D\\n\")\n",
    "    if 'full_mae' in locals():\n",
    "        f.write(f\"  Fully optimized MAE: {full_mae:.3f} D\\n\")\n",
    "\n",
    "print(\"\\nFormula saved to:\")\n",
    "print(\"  - SRKT_FacoDMEK_optimized_formula.json (for implementation)\")\n",
    "print(\"  - SRKT_FacoDMEK_optimized_formula.txt (human-readable)\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"DOCUMENTATION COMPLETE!\")\n",
    "print(\"=\"*80)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
