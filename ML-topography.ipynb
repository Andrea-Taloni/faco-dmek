{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bb64766",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 1: Import Libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import LeaveOneOut, cross_val_score\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.metrics import mean_absolute_error, r2_score\n",
    "from scipy.optimize import minimize\n",
    "from scipy import stats\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "print(\"Libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "113d7601",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2: Load and Explore Data\n",
    "# Load the Excel file\n",
    "df = pd.read_excel('FacoDMEK.xlsx', sheet_name='Cleaned Data')\n",
    "\n",
    "# Display basic information\n",
    "print(f\"Dataset shape: {df.shape}\")\n",
    "print(f\"\\nColumns: {df.columns.tolist()}\")\n",
    "print(f\"\\nFirst few rows:\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "088e74bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 3: Data Preprocessing - Using Keratometry Values\n",
    "# Calculate average keratometry from KERATOMETRY (not biometry)\n",
    "df['K_avg_Kerato'] = (df['Keratometric Ks'] + df['Keratometric Kf']) / 2\n",
    "\n",
    "# Calculate the \"true\" IOL power that would have achieved emmetropia\n",
    "df['True_IOL'] = df['IOL Power'] - df['PostOP Spherical Equivalent']\n",
    "\n",
    "# Feature engineering for keratometry-based analysis\n",
    "df['K_Astigmatism_Kerato'] = df['Keratometric Ks'] - df['Keratometric Kf']\n",
    "df['Post_Ant_Ratio'] = df['Posterior Km'] / df['Anterior Km']\n",
    "\n",
    "# NOTE: We'll apply RobustScaler later, so no manual normalization here\n",
    "\n",
    "# Display summary statistics\n",
    "print(\"Summary of key variables:\")\n",
    "print(df[['Bio-AL', 'K_avg_Kerato', 'IOL Power', 'PostOP Spherical Equivalent', \n",
    "          'True_IOL', 'CCT', 'Posterior Km']].describe())\n",
    "\n",
    "# Check for missing values\n",
    "print(\"\\nMissing values in key columns:\")\n",
    "missing_counts = df[['Bio-AL', 'K_avg_Kerato', 'IOL Power', \n",
    "                     'PostOP Spherical Equivalent', 'True_IOL', 'CCT', \n",
    "                     'A-Constant', 'Posterior Km']].isnull().sum()\n",
    "print(missing_counts)\n",
    "\n",
    "# Count complete cases\n",
    "complete_cases = df[['Bio-AL', 'K_avg_Kerato', 'IOL Power', \n",
    "                     'PostOP Spherical Equivalent', 'A-Constant']].notna().all(axis=1).sum()\n",
    "print(f\"\\nComplete cases for analysis: {complete_cases} out of {len(df)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbb03cb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 4: Implement SRK/T Formula with Keratometry K Values\n",
    "def calculate_SRKT(AL, K, A_const, nc=1.333):\n",
    "    \"\"\"\n",
    "    Calculate IOL power using SRK/T formula\n",
    "    Uses keratometry K values\n",
    "    Returns NaN if inputs are invalid\n",
    "    \"\"\"\n",
    "    # Check for valid inputs\n",
    "    if pd.isna(AL) or pd.isna(K) or pd.isna(A_const) or K <= 0 or AL <= 0:\n",
    "        return np.nan\n",
    "    \n",
    "    try:\n",
    "        # Constants\n",
    "        na = 1.336\n",
    "        V = 12\n",
    "        \n",
    "        # Corneal radius from keratometry K\n",
    "        r = 337.5 / K\n",
    "        \n",
    "        # Axial length correction - USING NEGATIVE 3.446 AS PER EXCEL FORMULA\n",
    "        if AL > 24.2:\n",
    "            LCOR = -3.446 + 1.716 * AL - 0.0237 * AL**2\n",
    "        else:\n",
    "            LCOR = AL\n",
    "        \n",
    "        # Corneal width\n",
    "        Cw = -5.41 + 0.58412 * LCOR + 0.098 * K\n",
    "        \n",
    "        # Check if we can calculate H (avoid negative square root)\n",
    "        if r**2 - (Cw**2 / 4) < 0:\n",
    "            return np.nan\n",
    "        \n",
    "        # Corneal height\n",
    "        H = r - np.sqrt(r**2 - (Cw**2 / 4))\n",
    "        \n",
    "        # ACD constant from A-constant\n",
    "        ACDconst = 0.62467 * A_const - 68.747\n",
    "        \n",
    "        # Offset\n",
    "        offset = ACDconst - 3.336\n",
    "        \n",
    "        # Estimated postoperative ACD\n",
    "        ACDest = H + offset\n",
    "        \n",
    "        # Retinal thickness correction\n",
    "        RETHICK = 0.65696 - 0.02029 * AL\n",
    "        LOPT = AL + RETHICK\n",
    "        \n",
    "        # Calculate IOL power for emmetropia\n",
    "        ncm1 = nc - 1\n",
    "        IOL = (1000 * na * (na * r - ncm1 * LOPT)) / ((LOPT - ACDest) * (na * r - ncm1 * ACDest))\n",
    "        \n",
    "        return IOL\n",
    "    except:\n",
    "        return np.nan\n",
    "\n",
    "# Calculate SRK/T predictions using keratometry K values\n",
    "df['SRKT_Prediction'] = df.apply(\n",
    "    lambda row: calculate_SRKT(row['Bio-AL'], row['K_avg_Kerato'], row['A-Constant']), \n",
    "    axis=1\n",
    ")\n",
    "df['SRKT_Error'] = df['SRKT_Prediction'] - df['True_IOL']\n",
    "\n",
    "# Remove rows with NaN errors for analysis\n",
    "valid_cases = df['SRKT_Error'].notna()\n",
    "\n",
    "print(\"SRK/T with Keratometry K Values:\")\n",
    "print(f\"Valid predictions: {valid_cases.sum()} out of {len(df)}\")\n",
    "if valid_cases.sum() > 0:\n",
    "    print(f\"Mean Error: {df.loc[valid_cases, 'SRKT_Error'].mean():.3f} D\")\n",
    "    print(f\"Mean Absolute Error: {df.loc[valid_cases, 'SRKT_Error'].abs().mean():.3f} D\")\n",
    "    print(f\"Standard Deviation: {df.loc[valid_cases, 'SRKT_Error'].std():.3f} D\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9ff038c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 5: Visualize SRK/T Performance with Keratometry\n",
    "# Only plot for rows with valid data\n",
    "df_valid = df[df['SRKT_Error'].notna()].copy()\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(12, 10))\n",
    "\n",
    "# Plot 1: Error Distribution\n",
    "if len(df_valid) > 0:\n",
    "    axes[0, 0].hist(df_valid['SRKT_Error'], bins=20, edgecolor='black', alpha=0.7)\n",
    "    axes[0, 0].axvline(x=0, color='red', linestyle='--')\n",
    "    axes[0, 0].set_xlabel('SRK/T Prediction Error (D)')\n",
    "    axes[0, 0].set_ylabel('Frequency')\n",
    "    axes[0, 0].set_title(f'Error Distribution (n={len(df_valid)})')\n",
    "\n",
    "# Plot 2: Error vs Axial Length\n",
    "    axes[0, 1].scatter(df_valid['Bio-AL'], df_valid['SRKT_Error'], alpha=0.6)\n",
    "    axes[0, 1].set_xlabel('Axial Length (mm)')\n",
    "    axes[0, 1].set_ylabel('SRK/T Error (D)')\n",
    "    axes[0, 1].set_title('Error vs Axial Length')\n",
    "    axes[0, 1].axhline(y=0, color='red', linestyle='--', alpha=0.5)\n",
    "\n",
    "# Plot 3: Error vs Posterior Corneal Power\n",
    "    axes[1, 0].scatter(df_valid['Posterior Km'], df_valid['SRKT_Error'], alpha=0.6)\n",
    "    axes[1, 0].set_xlabel('Posterior Corneal Power (D)')\n",
    "    axes[1, 0].set_ylabel('SRK/T Error (D)')\n",
    "    axes[1, 0].set_title('Error vs Posterior K')\n",
    "    axes[1, 0].axhline(y=0, color='red', linestyle='--', alpha=0.5)\n",
    "\n",
    "# Plot 4: Error vs CCT\n",
    "    valid_cct = df_valid[df_valid['CCT'].notna()]\n",
    "    axes[1, 1].scatter(valid_cct['CCT'], valid_cct['SRKT_Error'], alpha=0.6)\n",
    "    axes[1, 1].set_xlabel('Central Corneal Thickness (μm)')\n",
    "    axes[1, 1].set_ylabel('SRK/T Error (D)')\n",
    "    axes[1, 1].set_title('Error vs CCT')\n",
    "    axes[1, 1].axhline(y=0, color='red', linestyle='--', alpha=0.5)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Calculate percentage within target ranges\n",
    "if len(df_valid) > 0:\n",
    "    within_025 = (df_valid['SRKT_Error'].abs() <= 0.25).sum() / len(df_valid) * 100\n",
    "    within_050 = (df_valid['SRKT_Error'].abs() <= 0.50).sum() / len(df_valid) * 100\n",
    "    within_100 = (df_valid['SRKT_Error'].abs() <= 1.00).sum() / len(df_valid) * 100\n",
    "    \n",
    "    print(f\"\\nPercentage of eyes within target:\")\n",
    "    print(f\"±0.25 D: {within_025:.1f}%\")\n",
    "    print(f\"±0.50 D: {within_050:.1f}%\")\n",
    "    print(f\"±1.00 D: {within_100:.1f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c261c01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 6: Optimization Approach 1 - Optimize Corneal Refractive Index\n",
    "# Only use complete cases\n",
    "df_complete = df[df['SRKT_Error'].notna()].copy()\n",
    "\n",
    "def objective_nc(nc_value):\n",
    "    \"\"\"Objective function to minimize MAE by optimizing nc\"\"\"\n",
    "    predictions = df_complete.apply(\n",
    "        lambda row: calculate_SRKT(row['Bio-AL'], row['K_avg_Kerato'], row['A-Constant'], nc=nc_value[0]), \n",
    "        axis=1\n",
    "    )\n",
    "    errors = predictions - df_complete['True_IOL']\n",
    "    # Remove any NaN values that might occur\n",
    "    valid_errors = errors[errors.notna()]\n",
    "    if len(valid_errors) == 0:\n",
    "        return 999  # Return large value if no valid predictions\n",
    "    return np.mean(np.abs(valid_errors))\n",
    "\n",
    "# Optimize nc\n",
    "if len(df_complete) > 0:\n",
    "    result_nc = minimize(objective_nc, x0=[1.333], bounds=[(1.330, 1.340)], method='L-BFGS-B')\n",
    "    optimal_nc = result_nc.x[0]\n",
    "    \n",
    "    # Recalculate with optimal nc\n",
    "    df_complete['SRKT_Optimized_nc'] = df_complete.apply(\n",
    "        lambda row: calculate_SRKT(row['Bio-AL'], row['K_avg_Kerato'], row['A-Constant'], nc=optimal_nc), \n",
    "        axis=1\n",
    "    )\n",
    "    df_complete['SRKT_Error_Optimized_nc'] = df_complete['SRKT_Optimized_nc'] - df_complete['True_IOL']\n",
    "    \n",
    "    print(f\"Optimal corneal refractive index: {optimal_nc:.4f}\")\n",
    "    print(f\"Original MAE: {df_complete['SRKT_Error'].abs().mean():.3f} D\")\n",
    "    print(f\"Optimized MAE: {df_complete['SRKT_Error_Optimized_nc'].abs().mean():.3f} D\")\n",
    "else:\n",
    "    print(\"Not enough complete cases for optimization\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0f35002",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 7: Linear Correction Model with RobustScaler\n",
    "# Prepare features for correction model\n",
    "feature_cols = ['Posterior Km', 'CCT', 'Post_Ant_Ratio', 'K_Astigmatism_Kerato']\n",
    "\n",
    "# Create a dataset with only complete cases\n",
    "df_ml = df.copy()\n",
    "\n",
    "# Ensure we have all necessary columns\n",
    "required_cols = feature_cols + ['SRKT_Error']\n",
    "df_ml = df_ml[df_ml[required_cols].notna().all(axis=1)].copy()\n",
    "\n",
    "print(f\"Complete cases for ML: {len(df_ml)} out of {len(df)}\")\n",
    "\n",
    "if len(df_ml) > 10:  # Need at least 10 cases for meaningful analysis\n",
    "    X = df_ml[feature_cols]\n",
    "    y = df_ml['SRKT_Error']\n",
    "    \n",
    "    # Initialize RobustScaler\n",
    "    scaler = RobustScaler()\n",
    "    \n",
    "    # Split data for cross-validation\n",
    "    loo = LeaveOneOut()\n",
    "    predictions_linear = []\n",
    "    true_values = []\n",
    "    \n",
    "    for train_index, test_index in loo.split(X):\n",
    "        X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "        y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "        \n",
    "        # Scale features using RobustScaler\n",
    "        scaler_cv = RobustScaler()\n",
    "        X_train_scaled = scaler_cv.fit_transform(X_train)\n",
    "        X_test_scaled = scaler_cv.transform(X_test)\n",
    "        \n",
    "        # Train linear model\n",
    "        model = LinearRegression()\n",
    "        model.fit(X_train_scaled, y_train)\n",
    "        \n",
    "        # Predict correction\n",
    "        correction = model.predict(X_test_scaled)[0]\n",
    "        \n",
    "        # Apply correction to SRK/T prediction\n",
    "        srkt_pred = df_ml.iloc[test_index]['SRKT_Prediction'].values[0]\n",
    "        corrected_pred = srkt_pred - correction\n",
    "        \n",
    "        predictions_linear.append(corrected_pred)\n",
    "        true_values.append(df_ml.iloc[test_index]['True_IOL'].values[0])\n",
    "    \n",
    "    # Calculate performance\n",
    "    mae_original = df_ml['SRKT_Error'].abs().mean()\n",
    "    mae_linear = mean_absolute_error(true_values, predictions_linear)\n",
    "    \n",
    "    print(f\"Original SRK/T MAE: {mae_original:.3f} D\")\n",
    "    print(f\"Linear Correction Model MAE: {mae_linear:.3f} D\")\n",
    "    print(f\"Improvement: {mae_original - mae_linear:.3f} D\")\n",
    "    \n",
    "    # Show feature importance with scaled data\n",
    "    X_scaled_full = scaler.fit_transform(X)\n",
    "    model_full = LinearRegression()\n",
    "    model_full.fit(X_scaled_full, y)\n",
    "    \n",
    "    feature_importance = pd.DataFrame({\n",
    "        'Feature': feature_cols,\n",
    "        'Coefficient': model_full.coef_,\n",
    "        'Abs_Coefficient': np.abs(model_full.coef_)\n",
    "    }).sort_values('Abs_Coefficient', ascending=False)\n",
    "    \n",
    "    print(\"\\nFeature Importance (Linear Model with RobustScaler):\")\n",
    "    print(feature_importance)\n",
    "else:\n",
    "    print(\"Not enough complete cases for machine learning analysis\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a46fff5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 8: Machine Learning Models Comparison with RobustScaler\n",
    "if len(df_ml) > 10:\n",
    "    # Prepare all keratometry-based features\n",
    "    features_ml = ['Bio-AL', 'K_avg_Kerato', 'Posterior Km', 'CCT', 'Post_Ant_Ratio', \n",
    "                   'K_Astigmatism_Kerato', 'A-Constant']\n",
    "    \n",
    "    # Ensure complete cases\n",
    "    df_ml_full = df_ml[features_ml + ['True_IOL']].dropna()\n",
    "    X_ml = df_ml_full[features_ml]\n",
    "    y_ml = df_ml_full['True_IOL']\n",
    "    \n",
    "    print(f\"Cases for ML comparison: {len(X_ml)}\")\n",
    "    \n",
    "    # Initialize models\n",
    "    models = {\n",
    "        'Linear Regression': LinearRegression(),\n",
    "        'Ridge Regression': Ridge(alpha=1.0),\n",
    "        'Random Forest': RandomForestRegressor(n_estimators=100, max_depth=5, random_state=42),\n",
    "        'Gradient Boosting': GradientBoostingRegressor(n_estimators=100, max_depth=3, random_state=42)\n",
    "    }\n",
    "    \n",
    "    # Cross-validation results with RobustScaler\n",
    "    results = {}\n",
    "    \n",
    "    for name, model in models.items():\n",
    "        predictions = []\n",
    "        actuals = []\n",
    "        \n",
    "        for train_idx, test_idx in loo.split(X_ml):\n",
    "            X_train, X_test = X_ml.iloc[train_idx], X_ml.iloc[test_idx]\n",
    "            y_train, y_test = y_ml.iloc[train_idx], y_ml.iloc[test_idx]\n",
    "            \n",
    "            # Apply RobustScaler\n",
    "            scaler = RobustScaler()\n",
    "            X_train_scaled = scaler.fit_transform(X_train)\n",
    "            X_test_scaled = scaler.transform(X_test)\n",
    "            \n",
    "            model.fit(X_train_scaled, y_train)\n",
    "            pred = model.predict(X_test_scaled)[0]\n",
    "            \n",
    "            predictions.append(pred)\n",
    "            actuals.append(y_test.values[0])\n",
    "        \n",
    "        mae = mean_absolute_error(actuals, predictions)\n",
    "        results[name] = mae\n",
    "        print(f\"{name} MAE: {mae:.3f} D\")\n",
    "    \n",
    "    # Compare with original SRK/T\n",
    "    original_mae = df_ml_full.merge(df[['SRKT_Error']], left_index=True, right_index=True)['SRKT_Error'].abs().mean()\n",
    "    print(f\"\\nOriginal SRK/T MAE: {original_mae:.3f} D\")\n",
    "    print(f\"Best ML Model: {min(results, key=results.get)} with MAE: {min(results.values()):.3f} D\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3954664",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 9: Create Final Optimized Formula for FacoDMEK with RobustScaler\n",
    "if len(df_ml) > 10:\n",
    "    # Prepare and scale all data\n",
    "    scaler_final = RobustScaler()\n",
    "    X_ml_scaled = scaler_final.fit_transform(X_ml)\n",
    "    \n",
    "    # Train final model on all scaled data\n",
    "    best_model = GradientBoostingRegressor(n_estimators=100, max_depth=3, random_state=42)\n",
    "    best_model.fit(X_ml_scaled, y_ml)\n",
    "    \n",
    "    # Feature importance\n",
    "    feature_importance_gb = pd.DataFrame({\n",
    "        'Feature': features_ml,\n",
    "        'Importance': best_model.feature_importances_\n",
    "    }).sort_values('Importance', ascending=False)\n",
    "    \n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.barh(feature_importance_gb['Feature'], feature_importance_gb['Importance'])\n",
    "    plt.xlabel('Feature Importance')\n",
    "    plt.title('Feature Importance in IOL Power Prediction Model (with RobustScaler)')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"Feature Importance:\")\n",
    "    print(feature_importance_gb)\n",
    "    \n",
    "    # Create simplified correction formula based on key features\n",
    "    top_features = ['Posterior Km', 'CCT', 'Bio-AL']\n",
    "    \n",
    "    # Ensure we have complete data for these features\n",
    "    df_simple = df_ml[top_features + ['True_IOL', 'SRKT_Prediction']].dropna()\n",
    "    X_simple = df_simple[top_features]\n",
    "    y_correction = df_simple['True_IOL'] - df_simple['SRKT_Prediction']\n",
    "    \n",
    "    # Scale the features\n",
    "    scaler_simple = RobustScaler()\n",
    "    X_simple_scaled = scaler_simple.fit_transform(X_simple)\n",
    "    \n",
    "    model_simple = LinearRegression()\n",
    "    model_simple.fit(X_simple_scaled, y_correction)\n",
    "    \n",
    "    # Get the scaling parameters for clinical use\n",
    "    print(f\"\\nRobustScaler Parameters (for clinical implementation):\")\n",
    "    print(f\"Medians: {scaler_simple.center_}\")\n",
    "    print(f\"IQRs: {scaler_simple.scale_}\")\n",
    "    \n",
    "    print(f\"\\nFacoDMEK Correction Formula (with RobustScaler):\")\n",
    "    print(f\"Step 1: Scale features using RobustScaler\")\n",
    "    for i, feat in enumerate(top_features):\n",
    "        print(f\"  {feat}_scaled = ({feat} - {scaler_simple.center_[i]:.3f}) / {scaler_simple.scale_[i]:.3f}\")\n",
    "    \n",
    "    print(f\"\\nStep 2: Calculate correction\")\n",
    "    print(f\"Correction = {model_simple.intercept_:.3f}\")\n",
    "    for feat, coef in zip(top_features, model_simple.coef_):\n",
    "        print(f\"           + {coef:.4f} × {feat}_scaled\")\n",
    "    \n",
    "    # Calculate final performance\n",
    "    df_simple['Predicted_Correction'] = model_simple.predict(X_simple_scaled)\n",
    "    df_simple['Final_Prediction'] = df_simple['SRKT_Prediction'] + df_simple['Predicted_Correction']\n",
    "    final_mae = mean_absolute_error(df_simple['True_IOL'], df_simple['Final_Prediction'])\n",
    "    \n",
    "    print(f\"\\nFinal formula MAE: {final_mae:.3f} D\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4e3561a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 10: Final Validation and Results Summary\n",
    "if len(df_ml) > 10:\n",
    "    # Summary statistics\n",
    "    print(\"FINAL RESULTS SUMMARY - KERATOMETRY-BASED ANALYSIS WITH ROBUSTSCALER\")\n",
    "    print(\"=\"*60)\n",
    "    print(f\"Original data points: {len(df)}\")\n",
    "    print(f\"Complete cases for analysis: {len(df_ml)}\")\n",
    "    print(f\"\\nOriginal SRK/T Performance (Keratometry K):\")\n",
    "    print(f\"  MAE: {df_ml['SRKT_Error'].abs().mean():.3f} D\")\n",
    "    print(f\"  Mean Error: {df_ml['SRKT_Error'].mean():.3f} D\")\n",
    "    print(f\"  STD: {df_ml['SRKT_Error'].std():.3f} D\")\n",
    "    \n",
    "    if 'optimal_nc' in locals():\n",
    "        print(f\"\\nOptimized corneal refractive index: {optimal_nc:.4f}\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"RECOMMENDED FACODMEK FORMULA (WITH ROBUSTSCALER):\")\n",
    "    print(\"=\"*60)\n",
    "    print(\"Modified_IOL = SRK/T_Keratometry + Correction\")\n",
    "    print(\"\\nWhere Correction is calculated after RobustScaler normalization\")\n",
    "    \n",
    "    # Create a practical clinical formula card\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"CLINICAL IMPLEMENTATION:\")\n",
    "    print(\"=\"*60)\n",
    "    print(\"1. Calculate standard SRK/T using keratometry K\")\n",
    "    print(\"2. Normalize features using RobustScaler parameters from your population:\")\n",
    "    for i, feat in enumerate(top_features):\n",
    "        print(f\"   {feat}_scaled = ({feat} - {scaler_simple.center_[i]:.2f}) / {scaler_simple.scale_[i]:.2f}\")\n",
    "    print(f\"3. Correction = {model_simple.intercept_:.3f} + \"\n",
    "          f\"{model_simple.coef_[0]:.3f}×PostK_scaled + \"\n",
    "          f\"{model_simple.coef_[1]:.4f}×CCT_scaled + \"\n",
    "          f\"{model_simple.coef_[2]:.3f}×AL_scaled\")\n",
    "    print(\"4. Modified IOL = SRK/T + Correction\")\n",
    "    \n",
    "    print(\"\\nNote: RobustScaler ensures robustness to outliers common in FacoDMEK eyes\")\n",
    "    \n",
    "    # Save results with scaling parameters\n",
    "    results_df = df_ml[['ID', 'Patient', 'Eye', 'Bio-AL', 'K_avg_Kerato', 'IOL Power', \n",
    "                        'PostOP Spherical Equivalent', 'True_IOL', 'SRKT_Prediction', \n",
    "                        'SRKT_Error', 'Posterior Km', 'CCT']].copy()\n",
    "    \n",
    "    # Add scaling parameters to the Excel file\n",
    "    scaling_params_df = pd.DataFrame({\n",
    "        'Feature': top_features,\n",
    "        'Median': scaler_simple.center_,\n",
    "        'IQR': scaler_simple.scale_\n",
    "    })\n",
    "    \n",
    "    # Save both dataframes to Excel\n",
    "    with pd.ExcelWriter('FacoDMEK_RobustScaler_Results.xlsx') as writer:\n",
    "        results_df.to_excel(writer, sheet_name='Results', index=False)\n",
    "        scaling_params_df.to_excel(writer, sheet_name='Scaling_Parameters', index=False)\n",
    "    \n",
    "    print(\"\\nResults saved to 'FacoDMEK_RobustScaler_Results.xlsx'\")\n",
    "    \n",
    "    # Create visualization of improvement\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    \n",
    "    # Bland-Altman plot\n",
    "    mean_values = (df_simple['True_IOL'] + df_simple['Final_Prediction']) / 2\n",
    "    diff_values = df_simple['Final_Prediction'] - df_simple['True_IOL']\n",
    "    \n",
    "    plt.scatter(mean_values, diff_values, alpha=0.6)\n",
    "    plt.axhline(y=0, color='red', linestyle='-', linewidth=2)\n",
    "    plt.axhline(y=diff_values.mean(), color='blue', linestyle='--')\n",
    "    plt.axhline(y=diff_values.mean() + 1.96*diff_values.std(), color='blue', linestyle=':')\n",
    "    plt.axhline(y=diff_values.mean() - 1.96*diff_values.std(), color='blue', linestyle=':')\n",
    "    \n",
    "    plt.xlabel('Mean of True and Predicted IOL (D)')\n",
    "    plt.ylabel('Predicted - True IOL (D)')\n",
    "    plt.title('Bland-Altman Plot: FacoDMEK Formula Performance (with RobustScaler)')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "else:\n",
    "    print(\"Insufficient data for complete analysis\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a28c1d7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 11: Extract Formula from Ridge Regression Model with RobustScaler\n",
    "if len(df_ml) > 10:\n",
    "    # Prepare data\n",
    "    df_ridge = df_ml[features_ml + ['True_IOL', 'SRKT_Prediction']].dropna()\n",
    "    X_ridge = df_ridge[features_ml]\n",
    "    \n",
    "    # Calculate the correction needed (True IOL - SRK/T prediction)\n",
    "    y_correction = df_ridge['True_IOL'] - df_ridge['SRKT_Prediction']\n",
    "    \n",
    "    # Apply RobustScaler\n",
    "    scaler_ridge = RobustScaler()\n",
    "    X_ridge_scaled = scaler_ridge.fit_transform(X_ridge)\n",
    "    \n",
    "    # Train Ridge Regression for correction\n",
    "    ridge_model = Ridge(alpha=1.0)\n",
    "    ridge_model.fit(X_ridge_scaled, y_correction)\n",
    "    \n",
    "    # Extract coefficients\n",
    "    print(\"RIDGE REGRESSION CORRECTION FORMULA (WITH ROBUSTSCALER):\")\n",
    "    print(\"=\"*60)\n",
    "    print(f\"Correction = {ridge_model.intercept_:.4f}\")\n",
    "    \n",
    "    # Create a coefficient table\n",
    "    coef_df = pd.DataFrame({\n",
    "        'Feature': features_ml,\n",
    "        'Coefficient': ridge_model.coef_,\n",
    "        'Abs_Coefficient': np.abs(ridge_model.coef_),\n",
    "        'Median': scaler_ridge.center_,\n",
    "        'IQR': scaler_ridge.scale_\n",
    "    }).sort_values('Abs_Coefficient', ascending=False)\n",
    "    \n",
    "    print(\"\\nCoefficients and Scaling Parameters:\")\n",
    "    print(coef_df)\n",
    "    \n",
    "    # Create the practical formula\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"PRACTICAL FACODMEK FORMULA FROM RIDGE REGRESSION:\")\n",
    "    print(\"=\"*60)\n",
    "    print(\"Modified_IOL = Standard_SRK/T + Correction\")\n",
    "    print(f\"\\nWhere Correction = {ridge_model.intercept_:.3f}\")\n",
    "    \n",
    "    print(\"\\nAfter scaling each feature:\")\n",
    "    for i, (feat, coef) in enumerate(zip(features_ml, ridge_model.coef_)):\n",
    "        if abs(coef) > 0.001:  # Only show meaningful coefficients\n",
    "            print(f\"    + ({coef:.4f} × [{feat} - {scaler_ridge.center_[i]:.2f}] / {scaler_ridge.scale_[i]:.2f})\")\n",
    "    \n",
    "    # Simplified version using only the most important features\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"SIMPLIFIED FORMULA (Top 3 Features):\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Get top 3 features by absolute coefficient value\n",
    "    top_3_indices = coef_df.nlargest(3, 'Abs_Coefficient').index\n",
    "    top_3_features = [features_ml[i] for i in top_3_indices]\n",
    "    top_3_coefs = [ridge_model.coef_[i] for i in top_3_indices]\n",
    "    \n",
    "    # Re-fit with only top 3 features for a cleaner formula\n",
    "    X_simple_ridge = df_ridge[top_3_features]\n",
    "    scaler_simple_ridge = RobustScaler()\n",
    "    X_simple_ridge_scaled = scaler_simple_ridge.fit_transform(X_simple_ridge)\n",
    "    \n",
    "    ridge_simple = Ridge(alpha=1.0)\n",
    "    ridge_simple.fit(X_simple_ridge_scaled, y_correction)\n",
    "    \n",
    "    print(\"Modified_IOL = Standard_SRK/T + Correction\")\n",
    "    print(f\"\\nWhere Correction = {ridge_simple.intercept_:.3f}\")\n",
    "    \n",
    "    print(\"\\nScaling parameters for clinical use:\")\n",
    "    for i, feat in enumerate(top_3_features):\n",
    "        print(f\"{feat}: median={scaler_simple_ridge.center_[i]:.2f}, IQR={scaler_simple_ridge.scale_[i]:.2f}\")\n",
    "    \n",
    "    print(\"\\nFinal correction formula:\")\n",
    "    for i, (feat, coef) in enumerate(zip(top_3_features, ridge_simple.coef_)):\n",
    "        print(f\"    + ({coef:.4f} × [{feat} - {scaler_simple_ridge.center_[i]:.2f}] / {scaler_simple_ridge.scale_[i]:.2f})\")\n",
    "    \n",
    "    # Validate the simplified formula\n",
    "    y_pred_simple = ridge_simple.predict(X_simple_ridge_scaled)\n",
    "    final_iol_simple = df_ridge['SRKT_Prediction'] + y_pred_simple\n",
    "    mae_simple = mean_absolute_error(df_ridge['True_IOL'], final_iol_simple)\n",
    "    \n",
    "    print(f\"\\nSimplified formula MAE: {mae_simple:.3f} D\")\n",
    "    \n",
    "    # Create a clinically usable formula card\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"CLINICAL FORMULA CARD - FACODMEK IOL CALCULATION\")\n",
    "    print(\"=\"*60)\n",
    "    print(\"Step 1: Calculate standard SRK/T using keratometry K\")\n",
    "    print(\"Step 2: Normalize features using population-specific parameters\")\n",
    "    print(\"Step 3: Apply correction formula\")\n",
    "    print(\"Step 4: Modified IOL = SRK/T + Correction\")\n",
    "    print(\"\\nIMPORTANT: RobustScaler parameters must be derived from YOUR population\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18234686",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 12: Advanced Machine Learning Models for Better Performance\n",
    "# Add this cell after Cell 11 in your notebook\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import LeaveOneOut, GridSearchCV, cross_val_score\n",
    "from sklearn.preprocessing import StandardScaler, PolynomialFeatures\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import mean_absolute_error, r2_score\n",
    "from sklearn.feature_selection import SelectKBest, f_regression, RFECV\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.ensemble import (RandomForestRegressor, GradientBoostingRegressor, \n",
    "                            ExtraTreesRegressor, VotingRegressor, StackingRegressor)\n",
    "from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "from sklearn.gaussian_process.kernels import RBF, WhiteKernel, Matern\n",
    "from sklearn.linear_model import (ElasticNet, HuberRegressor, RANSACRegressor, \n",
    "                                TheilSenRegressor, BayesianRidge, ARDRegression)\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "from sklearn.kernel_ridge import KernelRidge\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Ensure we have the data ready\n",
    "if len(df_ml) > 10:\n",
    "    # Prepare features\n",
    "    features_ml = ['Bio-AL', 'K_avg_Kerato', 'Posterior Km', 'CCT', 'Post_Ant_Ratio', \n",
    "                   'K_Astigmatism_Kerato', 'A-Constant']\n",
    "    \n",
    "    # Get complete cases\n",
    "    df_ml_full = df_ml[features_ml + ['True_IOL']].dropna()\n",
    "    X_ml = df_ml_full[features_ml]\n",
    "    y_ml = df_ml_full['True_IOL']\n",
    "    \n",
    "    print(f\"Cases for advanced ML analysis: {len(X_ml)}\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # Initialize LOO cross-validation\n",
    "    loo = LeaveOneOut()\n",
    "    \n",
    "    # Dictionary to store results\n",
    "    advanced_results = {}\n",
    "    \n",
    "    # 1. FEATURE ENGINEERING - Create polynomial and interaction features\n",
    "    print(\"1. Testing with Polynomial Features...\")\n",
    "    poly_features = PolynomialFeatures(degree=2, include_bias=False)\n",
    "    X_poly = poly_features.fit_transform(X_ml)\n",
    "    \n",
    "    # Ridge with polynomial features\n",
    "    ridge_poly = Ridge(alpha=1.0)\n",
    "    predictions = []\n",
    "    actuals = []\n",
    "    for train_idx, test_idx in loo.split(X_poly):\n",
    "        X_train, X_test = X_poly[train_idx], X_poly[test_idx]\n",
    "        y_train, y_test = y_ml.iloc[train_idx], y_ml.iloc[test_idx]\n",
    "        ridge_poly.fit(X_train, y_train)\n",
    "        predictions.append(ridge_poly.predict(X_test)[0])\n",
    "        actuals.append(y_test.values[0])\n",
    "    mae_poly = mean_absolute_error(actuals, predictions)\n",
    "    advanced_results['Ridge + Polynomial Features'] = mae_poly\n",
    "    print(f\"Ridge + Polynomial Features MAE: {mae_poly:.3f} D\")\n",
    "    \n",
    "    # 2. ROBUST REGRESSION METHODS\n",
    "    print(\"\\n2. Testing Robust Regression Methods...\")\n",
    "    \n",
    "    # Huber Regressor - robust to outliers\n",
    "    huber = HuberRegressor(epsilon=1.35)\n",
    "    predictions = []\n",
    "    actuals = []\n",
    "    for train_idx, test_idx in loo.split(X_ml):\n",
    "        X_train, X_test = X_ml.iloc[train_idx], X_ml.iloc[test_idx]\n",
    "        y_train, y_test = y_ml.iloc[train_idx], y_ml.iloc[test_idx]\n",
    "        huber.fit(X_train, y_train)\n",
    "        predictions.append(huber.predict(X_test)[0])\n",
    "        actuals.append(y_test.values[0])\n",
    "    mae_huber = mean_absolute_error(actuals, predictions)\n",
    "    advanced_results['Huber Regressor'] = mae_huber\n",
    "    print(f\"Huber Regressor MAE: {mae_huber:.3f} D\")\n",
    "    \n",
    "    # RANSAC - removes outliers automatically\n",
    "    ransac = RANSACRegressor(random_state=42)\n",
    "    predictions = []\n",
    "    actuals = []\n",
    "    for train_idx, test_idx in loo.split(X_ml):\n",
    "        X_train, X_test = X_ml.iloc[train_idx], X_ml.iloc[test_idx]\n",
    "        y_train, y_test = y_ml.iloc[train_idx], y_ml.iloc[test_idx]\n",
    "        ransac.fit(X_train, y_train)\n",
    "        predictions.append(ransac.predict(X_test)[0])\n",
    "        actuals.append(y_test.values[0])\n",
    "    mae_ransac = mean_absolute_error(actuals, predictions)\n",
    "    advanced_results['RANSAC'] = mae_ransac\n",
    "    print(f\"RANSAC MAE: {mae_ransac:.3f} D\")\n",
    "    \n",
    "    # 3. BAYESIAN METHODS\n",
    "    print(\"\\n3. Testing Bayesian Methods...\")\n",
    "    \n",
    "    # Bayesian Ridge\n",
    "    bayesian_ridge = BayesianRidge()\n",
    "    predictions = []\n",
    "    actuals = []\n",
    "    for train_idx, test_idx in loo.split(X_ml):\n",
    "        X_train, X_test = X_ml.iloc[train_idx], X_ml.iloc[test_idx]\n",
    "        y_train, y_test = y_ml.iloc[train_idx], y_ml.iloc[test_idx]\n",
    "        bayesian_ridge.fit(X_train, y_train)\n",
    "        predictions.append(bayesian_ridge.predict(X_test)[0])\n",
    "        actuals.append(y_test.values[0])\n",
    "    mae_bayesian = mean_absolute_error(actuals, predictions)\n",
    "    advanced_results['Bayesian Ridge'] = mae_bayesian\n",
    "    print(f\"Bayesian Ridge MAE: {mae_bayesian:.3f} D\")\n",
    "    \n",
    "    # 4. SUPPORT VECTOR REGRESSION with different kernels\n",
    "    print(\"\\n4. Testing Support Vector Regression...\")\n",
    "    \n",
    "    # Scale features for SVR\n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(X_ml)\n",
    "    \n",
    "    # SVR with RBF kernel\n",
    "    svr_rbf = SVR(kernel='rbf', C=10, gamma='scale')\n",
    "    predictions = []\n",
    "    actuals = []\n",
    "    for train_idx, test_idx in loo.split(X_scaled):\n",
    "        X_train, X_test = X_scaled[train_idx], X_scaled[test_idx]\n",
    "        y_train, y_test = y_ml.iloc[train_idx], y_ml.iloc[test_idx]\n",
    "        svr_rbf.fit(X_train, y_train)\n",
    "        predictions.append(svr_rbf.predict(X_test)[0])\n",
    "        actuals.append(y_test.values[0])\n",
    "    mae_svr_rbf = mean_absolute_error(actuals, predictions)\n",
    "    advanced_results['SVR (RBF)'] = mae_svr_rbf\n",
    "    print(f\"SVR (RBF kernel) MAE: {mae_svr_rbf:.3f} D\")\n",
    "    \n",
    "    # 5. GRADIENT BOOSTING VARIANTS\n",
    "    print(\"\\n5. Testing Advanced Gradient Boosting...\")\n",
    "    \n",
    "    # XGBoost\n",
    "    xgb_model = xgb.XGBRegressor(\n",
    "        n_estimators=100,\n",
    "        max_depth=3,\n",
    "        learning_rate=0.1,\n",
    "        subsample=0.8,\n",
    "        colsample_bytree=0.8,\n",
    "        random_state=42\n",
    "    )\n",
    "    predictions = []\n",
    "    actuals = []\n",
    "    for train_idx, test_idx in loo.split(X_ml):\n",
    "        X_train, X_test = X_ml.iloc[train_idx], X_ml.iloc[test_idx]\n",
    "        y_train, y_test = y_ml.iloc[train_idx], y_ml.iloc[test_idx]\n",
    "        xgb_model.fit(X_train, y_train)\n",
    "        predictions.append(xgb_model.predict(X_test)[0])\n",
    "        actuals.append(y_test.values[0])\n",
    "    mae_xgb = mean_absolute_error(actuals, predictions)\n",
    "    advanced_results['XGBoost'] = mae_xgb\n",
    "    print(f\"XGBoost MAE: {mae_xgb:.3f} D\")\n",
    "    \n",
    "    # LightGBM\n",
    "    lgb_model = lgb.LGBMRegressor(\n",
    "        n_estimators=100,\n",
    "        max_depth=3,\n",
    "        learning_rate=0.1,\n",
    "        num_leaves=31,\n",
    "        random_state=42\n",
    "    )\n",
    "    predictions = []\n",
    "    actuals = []\n",
    "    for train_idx, test_idx in loo.split(X_ml):\n",
    "        X_train, X_test = X_ml.iloc[train_idx], X_ml.iloc[test_idx]\n",
    "        y_train, y_test = y_ml.iloc[train_idx], y_ml.iloc[test_idx]\n",
    "        lgb_model.fit(X_train, y_train)\n",
    "        predictions.append(lgb_model.predict(X_test)[0])\n",
    "        actuals.append(y_test.values[0])\n",
    "    mae_lgb = mean_absolute_error(actuals, predictions)\n",
    "    advanced_results['LightGBM'] = mae_lgb\n",
    "    print(f\"LightGBM MAE: {mae_lgb:.3f} D\")\n",
    "    \n",
    "    # 6. NEURAL NETWORK\n",
    "    print(\"\\n6. Testing Neural Network...\")\n",
    "    \n",
    "    mlp = MLPRegressor(\n",
    "        hidden_layer_sizes=(50, 30),\n",
    "        activation='relu',\n",
    "        solver='adam',\n",
    "        alpha=0.01,\n",
    "        max_iter=1000,\n",
    "        random_state=42\n",
    "    )\n",
    "    predictions = []\n",
    "    actuals = []\n",
    "    for train_idx, test_idx in loo.split(X_scaled):\n",
    "        X_train, X_test = X_scaled[train_idx], X_scaled[test_idx]\n",
    "        y_train, y_test = y_ml.iloc[train_idx], y_ml.iloc[test_idx]\n",
    "        mlp.fit(X_train, y_train)\n",
    "        predictions.append(mlp.predict(X_test)[0])\n",
    "        actuals.append(y_test.values[0])\n",
    "    mae_mlp = mean_absolute_error(actuals, predictions)\n",
    "    advanced_results['Neural Network'] = mae_mlp\n",
    "    print(f\"Neural Network MAE: {mae_mlp:.3f} D\")\n",
    "    \n",
    "    # 7. ENSEMBLE METHODS\n",
    "    print(\"\\n7. Testing Ensemble Methods...\")\n",
    "    \n",
    "    # Voting Regressor - combines multiple models\n",
    "    voting_regressor = VotingRegressor([\n",
    "        ('ridge', Ridge(alpha=1.0)),\n",
    "        ('huber', HuberRegressor()),\n",
    "        ('gb', GradientBoostingRegressor(n_estimators=50, max_depth=3, random_state=42))\n",
    "    ])\n",
    "    predictions = []\n",
    "    actuals = []\n",
    "    for train_idx, test_idx in loo.split(X_ml):\n",
    "        X_train, X_test = X_ml.iloc[train_idx], X_ml.iloc[test_idx]\n",
    "        y_train, y_test = y_ml.iloc[train_idx], y_ml.iloc[test_idx]\n",
    "        voting_regressor.fit(X_train, y_train)\n",
    "        predictions.append(voting_regressor.predict(X_test)[0])\n",
    "        actuals.append(y_test.values[0])\n",
    "    mae_voting = mean_absolute_error(actuals, predictions)\n",
    "    advanced_results['Voting Ensemble'] = mae_voting\n",
    "    print(f\"Voting Ensemble MAE: {mae_voting:.3f} D\")\n",
    "    \n",
    "    # 8. GAUSSIAN PROCESS REGRESSION\n",
    "    print(\"\\n8. Testing Gaussian Process Regression...\")\n",
    "    \n",
    "    kernel = RBF(length_scale=1.0) + WhiteKernel(noise_level=1e-2)\n",
    "    gpr = GaussianProcessRegressor(kernel=kernel, n_restarts_optimizer=5, random_state=42)\n",
    "    predictions = []\n",
    "    actuals = []\n",
    "    for train_idx, test_idx in loo.split(X_scaled):\n",
    "        X_train, X_test = X_scaled[train_idx], X_scaled[test_idx]\n",
    "        y_train, y_test = y_ml.iloc[train_idx], y_ml.iloc[test_idx]\n",
    "        gpr.fit(X_train, y_train)\n",
    "        predictions.append(gpr.predict(X_test)[0])\n",
    "        actuals.append(y_test.values[0])\n",
    "    mae_gpr = mean_absolute_error(actuals, predictions)\n",
    "    advanced_results['Gaussian Process'] = mae_gpr\n",
    "    print(f\"Gaussian Process MAE: {mae_gpr:.3f} D\")\n",
    "    \n",
    "    # 9. ELASTIC NET with optimization\n",
    "    print(\"\\n9. Testing Elastic Net with optimization...\")\n",
    "    \n",
    "    elastic_net = ElasticNet(alpha=0.1, l1_ratio=0.5)\n",
    "    predictions = []\n",
    "    actuals = []\n",
    "    for train_idx, test_idx in loo.split(X_ml):\n",
    "        X_train, X_test = X_ml.iloc[train_idx], X_ml.iloc[test_idx]\n",
    "        y_train, y_test = y_ml.iloc[train_idx], y_ml.iloc[test_idx]\n",
    "        elastic_net.fit(X_train, y_train)\n",
    "        predictions.append(elastic_net.predict(X_test)[0])\n",
    "        actuals.append(y_test.values[0])\n",
    "    mae_elastic = mean_absolute_error(actuals, predictions)\n",
    "    advanced_results['Elastic Net'] = mae_elastic\n",
    "    print(f\"Elastic Net MAE: {mae_elastic:.3f} D\")\n",
    "    \n",
    "    # 10. KERNEL RIDGE REGRESSION\n",
    "    print(\"\\n10. Testing Kernel Ridge Regression...\")\n",
    "    \n",
    "    kr = KernelRidge(alpha=1.0, kernel='rbf', gamma=0.1)\n",
    "    predictions = []\n",
    "    actuals = []\n",
    "    for train_idx, test_idx in loo.split(X_scaled):\n",
    "        X_train, X_test = X_scaled[train_idx], X_scaled[test_idx]\n",
    "        y_train, y_test = y_ml.iloc[train_idx], y_ml.iloc[test_idx]\n",
    "        kr.fit(X_train, y_train)\n",
    "        predictions.append(kr.predict(X_test)[0])\n",
    "        actuals.append(y_test.values[0])\n",
    "    mae_kr = mean_absolute_error(actuals, predictions)\n",
    "    advanced_results['Kernel Ridge'] = mae_kr\n",
    "    print(f\"Kernel Ridge MAE: {mae_kr:.3f} D\")\n",
    "    \n",
    "    # RESULTS SUMMARY\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"ADVANCED MODEL RESULTS SUMMARY\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # Sort results by MAE\n",
    "    sorted_results = sorted(advanced_results.items(), key=lambda x: x[1])\n",
    "    \n",
    "    print(\"\\nAll models ranked by performance:\")\n",
    "    for i, (model, mae) in enumerate(sorted_results, 1):\n",
    "        print(f\"{i}. {model}: {mae:.3f} D\")\n",
    "    \n",
    "    best_model_name = sorted_results[0][0]\n",
    "    best_mae = sorted_results[0][1]\n",
    "    \n",
    "    print(f\"\\nBEST MODEL: {best_model_name} with MAE: {best_mae:.3f} D\")\n",
    "    print(f\"Improvement over Ridge: {1.455 - best_mae:.3f} D\")\n",
    "    \n",
    "    # Create visualization\n",
    "    import matplotlib.pyplot as plt\n",
    "    \n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
    "    \n",
    "    # Bar plot of all models\n",
    "    models = [x[0] for x in sorted_results]\n",
    "    maes = [x[1] for x in sorted_results]\n",
    "    colors = ['green' if mae < 1.455 else 'red' for mae in maes]\n",
    "    \n",
    "    ax1.barh(models, maes, color=colors)\n",
    "    ax1.axvline(x=1.455, color='blue', linestyle='--', label='Original Ridge MAE')\n",
    "    ax1.set_xlabel('Mean Absolute Error (D)')\n",
    "    ax1.set_title('Model Performance Comparison')\n",
    "    ax1.legend()\n",
    "    \n",
    "    # Improvement plot\n",
    "    improvements = [1.455 - mae for mae in maes]\n",
    "    ax2.barh(models, improvements, color=['green' if imp > 0 else 'red' for imp in improvements])\n",
    "    ax2.axvline(x=0, color='black', linestyle='-')\n",
    "    ax2.set_xlabel('Improvement over Ridge (D)')\n",
    "    ax2.set_title('Improvement Analysis')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "else:\n",
    "    print(\"Not enough data for advanced analysis\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b14f677",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 12: Advanced Machine Learning Models for Better Performance\n",
    "# Add this cell after Cell 11 in your notebook\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import LeaveOneOut, GridSearchCV, cross_val_score\n",
    "from sklearn.preprocessing import StandardScaler, PolynomialFeatures\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import mean_absolute_error, r2_score\n",
    "from sklearn.feature_selection import SelectKBest, f_regression, RFECV\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.ensemble import (RandomForestRegressor, GradientBoostingRegressor, \n",
    "                            ExtraTreesRegressor, VotingRegressor, StackingRegressor)\n",
    "from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "from sklearn.gaussian_process.kernels import RBF, WhiteKernel, Matern\n",
    "from sklearn.linear_model import (ElasticNet, HuberRegressor, RANSACRegressor, \n",
    "                                TheilSenRegressor, BayesianRidge, ARDRegression)\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "from sklearn.kernel_ridge import KernelRidge\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Ensure we have the data ready\n",
    "if len(df_ml) > 10:\n",
    "    # Prepare features\n",
    "    features_ml = ['Bio-AL', 'K_avg_Kerato', 'Posterior Km', 'CCT', 'Post_Ant_Ratio', \n",
    "                   'K_Astigmatism_Kerato', 'A-Constant']\n",
    "    \n",
    "    # Get complete cases\n",
    "    df_ml_full = df_ml[features_ml + ['True_IOL']].dropna()\n",
    "    X_ml = df_ml_full[features_ml]\n",
    "    y_ml = df_ml_full['True_IOL']\n",
    "    \n",
    "    print(f\"Cases for advanced ML analysis: {len(X_ml)}\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # Initialize LOO cross-validation\n",
    "    loo = LeaveOneOut()\n",
    "    \n",
    "    # Dictionary to store results\n",
    "    advanced_results = {}\n",
    "    \n",
    "    # 1. FEATURE ENGINEERING - Create polynomial and interaction features\n",
    "    print(\"1. Testing with Polynomial Features...\")\n",
    "    poly_features = PolynomialFeatures(degree=2, include_bias=False)\n",
    "    X_poly = poly_features.fit_transform(X_ml)\n",
    "    \n",
    "    # Ridge with polynomial features\n",
    "    ridge_poly = Ridge(alpha=1.0)\n",
    "    predictions = []\n",
    "    actuals = []\n",
    "    for train_idx, test_idx in loo.split(X_poly):\n",
    "        X_train, X_test = X_poly[train_idx], X_poly[test_idx]\n",
    "        y_train, y_test = y_ml.iloc[train_idx], y_ml.iloc[test_idx]\n",
    "        ridge_poly.fit(X_train, y_train)\n",
    "        predictions.append(ridge_poly.predict(X_test)[0])\n",
    "        actuals.append(y_test.values[0])\n",
    "    mae_poly = mean_absolute_error(actuals, predictions)\n",
    "    advanced_results['Ridge + Polynomial Features'] = mae_poly\n",
    "    print(f\"Ridge + Polynomial Features MAE: {mae_poly:.3f} D\")\n",
    "    \n",
    "    # 2. ROBUST REGRESSION METHODS\n",
    "    print(\"\\n2. Testing Robust Regression Methods...\")\n",
    "    \n",
    "    # Huber Regressor - robust to outliers\n",
    "    huber = HuberRegressor(epsilon=1.35)\n",
    "    predictions = []\n",
    "    actuals = []\n",
    "    for train_idx, test_idx in loo.split(X_ml):\n",
    "        X_train, X_test = X_ml.iloc[train_idx], X_ml.iloc[test_idx]\n",
    "        y_train, y_test = y_ml.iloc[train_idx], y_ml.iloc[test_idx]\n",
    "        huber.fit(X_train, y_train)\n",
    "        predictions.append(huber.predict(X_test)[0])\n",
    "        actuals.append(y_test.values[0])\n",
    "    mae_huber = mean_absolute_error(actuals, predictions)\n",
    "    advanced_results['Huber Regressor'] = mae_huber\n",
    "    print(f\"Huber Regressor MAE: {mae_huber:.3f} D\")\n",
    "    \n",
    "    # RANSAC - removes outliers automatically\n",
    "    ransac = RANSACRegressor(random_state=42)\n",
    "    predictions = []\n",
    "    actuals = []\n",
    "    for train_idx, test_idx in loo.split(X_ml):\n",
    "        X_train, X_test = X_ml.iloc[train_idx], X_ml.iloc[test_idx]\n",
    "        y_train, y_test = y_ml.iloc[train_idx], y_ml.iloc[test_idx]\n",
    "        ransac.fit(X_train, y_train)\n",
    "        predictions.append(ransac.predict(X_test)[0])\n",
    "        actuals.append(y_test.values[0])\n",
    "    mae_ransac = mean_absolute_error(actuals, predictions)\n",
    "    advanced_results['RANSAC'] = mae_ransac\n",
    "    print(f\"RANSAC MAE: {mae_ransac:.3f} D\")\n",
    "    \n",
    "    # 3. BAYESIAN METHODS\n",
    "    print(\"\\n3. Testing Bayesian Methods...\")\n",
    "    \n",
    "    # Bayesian Ridge\n",
    "    bayesian_ridge = BayesianRidge()\n",
    "    predictions = []\n",
    "    actuals = []\n",
    "    for train_idx, test_idx in loo.split(X_ml):\n",
    "        X_train, X_test = X_ml.iloc[train_idx], X_ml.iloc[test_idx]\n",
    "        y_train, y_test = y_ml.iloc[train_idx], y_ml.iloc[test_idx]\n",
    "        bayesian_ridge.fit(X_train, y_train)\n",
    "        predictions.append(bayesian_ridge.predict(X_test)[0])\n",
    "        actuals.append(y_test.values[0])\n",
    "    mae_bayesian = mean_absolute_error(actuals, predictions)\n",
    "    advanced_results['Bayesian Ridge'] = mae_bayesian\n",
    "    print(f\"Bayesian Ridge MAE: {mae_bayesian:.3f} D\")\n",
    "    \n",
    "    # 4. SUPPORT VECTOR REGRESSION with different kernels\n",
    "    print(\"\\n4. Testing Support Vector Regression...\")\n",
    "    \n",
    "    # Scale features for SVR\n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(X_ml)\n",
    "    \n",
    "    # SVR with RBF kernel\n",
    "    svr_rbf = SVR(kernel='rbf', C=10, gamma='scale')\n",
    "    predictions = []\n",
    "    actuals = []\n",
    "    for train_idx, test_idx in loo.split(X_scaled):\n",
    "        X_train, X_test = X_scaled[train_idx], X_scaled[test_idx]\n",
    "        y_train, y_test = y_ml.iloc[train_idx], y_ml.iloc[test_idx]\n",
    "        svr_rbf.fit(X_train, y_train)\n",
    "        predictions.append(svr_rbf.predict(X_test)[0])\n",
    "        actuals.append(y_test.values[0])\n",
    "    mae_svr_rbf = mean_absolute_error(actuals, predictions)\n",
    "    advanced_results['SVR (RBF)'] = mae_svr_rbf\n",
    "    print(f\"SVR (RBF kernel) MAE: {mae_svr_rbf:.3f} D\")\n",
    "    \n",
    "    # 5. GRADIENT BOOSTING VARIANTS\n",
    "    print(\"\\n5. Testing Advanced Gradient Boosting...\")\n",
    "    \n",
    "    # XGBoost\n",
    "    xgb_model = xgb.XGBRegressor(\n",
    "        n_estimators=100,\n",
    "        max_depth=3,\n",
    "        learning_rate=0.1,\n",
    "        subsample=0.8,\n",
    "        colsample_bytree=0.8,\n",
    "        random_state=42\n",
    "    )\n",
    "    predictions = []\n",
    "    actuals = []\n",
    "    for train_idx, test_idx in loo.split(X_ml):\n",
    "        X_train, X_test = X_ml.iloc[train_idx], X_ml.iloc[test_idx]\n",
    "        y_train, y_test = y_ml.iloc[train_idx], y_ml.iloc[test_idx]\n",
    "        xgb_model.fit(X_train, y_train)\n",
    "        predictions.append(xgb_model.predict(X_test)[0])\n",
    "        actuals.append(y_test.values[0])\n",
    "    mae_xgb = mean_absolute_error(actuals, predictions)\n",
    "    advanced_results['XGBoost'] = mae_xgb\n",
    "    print(f\"XGBoost MAE: {mae_xgb:.3f} D\")\n",
    "    \n",
    "    # LightGBM\n",
    "    lgb_model = lgb.LGBMRegressor(\n",
    "        n_estimators=100,\n",
    "        max_depth=3,\n",
    "        learning_rate=0.1,\n",
    "        num_leaves=31,\n",
    "        random_state=42\n",
    "    )\n",
    "    predictions = []\n",
    "    actuals = []\n",
    "    for train_idx, test_idx in loo.split(X_ml):\n",
    "        X_train, X_test = X_ml.iloc[train_idx], X_ml.iloc[test_idx]\n",
    "        y_train, y_test = y_ml.iloc[train_idx], y_ml.iloc[test_idx]\n",
    "        lgb_model.fit(X_train, y_train)\n",
    "        predictions.append(lgb_model.predict(X_test)[0])\n",
    "        actuals.append(y_test.values[0])\n",
    "    mae_lgb = mean_absolute_error(actuals, predictions)\n",
    "    advanced_results['LightGBM'] = mae_lgb\n",
    "    print(f\"LightGBM MAE: {mae_lgb:.3f} D\")\n",
    "    \n",
    "    # 6. NEURAL NETWORK\n",
    "    print(\"\\n6. Testing Neural Network...\")\n",
    "    \n",
    "    mlp = MLPRegressor(\n",
    "        hidden_layer_sizes=(50, 30),\n",
    "        activation='relu',\n",
    "        solver='adam',\n",
    "        alpha=0.01,\n",
    "        max_iter=1000,\n",
    "        random_state=42\n",
    "    )\n",
    "    predictions = []\n",
    "    actuals = []\n",
    "    for train_idx, test_idx in loo.split(X_scaled):\n",
    "        X_train, X_test = X_scaled[train_idx], X_scaled[test_idx]\n",
    "        y_train, y_test = y_ml.iloc[train_idx], y_ml.iloc[test_idx]\n",
    "        mlp.fit(X_train, y_train)\n",
    "        predictions.append(mlp.predict(X_test)[0])\n",
    "        actuals.append(y_test.values[0])\n",
    "    mae_mlp = mean_absolute_error(actuals, predictions)\n",
    "    advanced_results['Neural Network'] = mae_mlp\n",
    "    print(f\"Neural Network MAE: {mae_mlp:.3f} D\")\n",
    "    \n",
    "    # 7. ENSEMBLE METHODS\n",
    "    print(\"\\n7. Testing Ensemble Methods...\")\n",
    "    \n",
    "    # Voting Regressor - combines multiple models\n",
    "    voting_regressor = VotingRegressor([\n",
    "        ('ridge', Ridge(alpha=1.0)),\n",
    "        ('huber', HuberRegressor()),\n",
    "        ('gb', GradientBoostingRegressor(n_estimators=50, max_depth=3, random_state=42))\n",
    "    ])\n",
    "    predictions = []\n",
    "    actuals = []\n",
    "    for train_idx, test_idx in loo.split(X_ml):\n",
    "        X_train, X_test = X_ml.iloc[train_idx], X_ml.iloc[test_idx]\n",
    "        y_train, y_test = y_ml.iloc[train_idx], y_ml.iloc[test_idx]\n",
    "        voting_regressor.fit(X_train, y_train)\n",
    "        predictions.append(voting_regressor.predict(X_test)[0])\n",
    "        actuals.append(y_test.values[0])\n",
    "    mae_voting = mean_absolute_error(actuals, predictions)\n",
    "    advanced_results['Voting Ensemble'] = mae_voting\n",
    "    print(f\"Voting Ensemble MAE: {mae_voting:.3f} D\")\n",
    "    \n",
    "    # 8. GAUSSIAN PROCESS REGRESSION\n",
    "    print(\"\\n8. Testing Gaussian Process Regression...\")\n",
    "    \n",
    "    kernel = RBF(length_scale=1.0) + WhiteKernel(noise_level=1e-2)\n",
    "    gpr = GaussianProcessRegressor(kernel=kernel, n_restarts_optimizer=5, random_state=42)\n",
    "    predictions = []\n",
    "    actuals = []\n",
    "    for train_idx, test_idx in loo.split(X_scaled):\n",
    "        X_train, X_test = X_scaled[train_idx], X_scaled[test_idx]\n",
    "        y_train, y_test = y_ml.iloc[train_idx], y_ml.iloc[test_idx]\n",
    "        gpr.fit(X_train, y_train)\n",
    "        predictions.append(gpr.predict(X_test)[0])\n",
    "        actuals.append(y_test.values[0])\n",
    "    mae_gpr = mean_absolute_error(actuals, predictions)\n",
    "    advanced_results['Gaussian Process'] = mae_gpr\n",
    "    print(f\"Gaussian Process MAE: {mae_gpr:.3f} D\")\n",
    "    \n",
    "    # 9. ELASTIC NET with optimization\n",
    "    print(\"\\n9. Testing Elastic Net with optimization...\")\n",
    "    \n",
    "    elastic_net = ElasticNet(alpha=0.1, l1_ratio=0.5)\n",
    "    predictions = []\n",
    "    actuals = []\n",
    "    for train_idx, test_idx in loo.split(X_ml):\n",
    "        X_train, X_test = X_ml.iloc[train_idx], X_ml.iloc[test_idx]\n",
    "        y_train, y_test = y_ml.iloc[train_idx], y_ml.iloc[test_idx]\n",
    "        elastic_net.fit(X_train, y_train)\n",
    "        predictions.append(elastic_net.predict(X_test)[0])\n",
    "        actuals.append(y_test.values[0])\n",
    "    mae_elastic = mean_absolute_error(actuals, predictions)\n",
    "    advanced_results['Elastic Net'] = mae_elastic\n",
    "    print(f\"Elastic Net MAE: {mae_elastic:.3f} D\")\n",
    "    \n",
    "    # 10. KERNEL RIDGE REGRESSION\n",
    "    print(\"\\n10. Testing Kernel Ridge Regression...\")\n",
    "    \n",
    "    kr = KernelRidge(alpha=1.0, kernel='rbf', gamma=0.1)\n",
    "    predictions = []\n",
    "    actuals = []\n",
    "    for train_idx, test_idx in loo.split(X_scaled):\n",
    "        X_train, X_test = X_scaled[train_idx], X_scaled[test_idx]\n",
    "        y_train, y_test = y_ml.iloc[train_idx], y_ml.iloc[test_idx]\n",
    "        kr.fit(X_train, y_train)\n",
    "        predictions.append(kr.predict(X_test)[0])\n",
    "        actuals.append(y_test.values[0])\n",
    "    mae_kr = mean_absolute_error(actuals, predictions)\n",
    "    advanced_results['Kernel Ridge'] = mae_kr\n",
    "    print(f\"Kernel Ridge MAE: {mae_kr:.3f} D\")\n",
    "    \n",
    "    # RESULTS SUMMARY\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"ADVANCED MODEL RESULTS SUMMARY\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # Sort results by MAE\n",
    "    sorted_results = sorted(advanced_results.items(), key=lambda x: x[1])\n",
    "    \n",
    "    print(\"\\nAll models ranked by performance:\")\n",
    "    for i, (model, mae) in enumerate(sorted_results, 1):\n",
    "        print(f\"{i}. {model}: {mae:.3f} D\")\n",
    "    \n",
    "    best_model_name = sorted_results[0][0]\n",
    "    best_mae = sorted_results[0][1]\n",
    "    \n",
    "    print(f\"\\nBEST MODEL: {best_model_name} with MAE: {best_mae:.3f} D\")\n",
    "    print(f\"Improvement over Ridge: {1.455 - best_mae:.3f} D\")\n",
    "    \n",
    "    # Create visualization\n",
    "    import matplotlib.pyplot as plt\n",
    "    \n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
    "    \n",
    "    # Bar plot of all models\n",
    "    models = [x[0] for x in sorted_results]\n",
    "    maes = [x[1] for x in sorted_results]\n",
    "    colors = ['green' if mae < 1.455 else 'red' for mae in maes]\n",
    "    \n",
    "    ax1.barh(models, maes, color=colors)\n",
    "    ax1.axvline(x=1.455, color='blue', linestyle='--', label='Original Ridge MAE')\n",
    "    ax1.set_xlabel('Mean Absolute Error (D)')\n",
    "    ax1.set_title('Model Performance Comparison')\n",
    "    ax1.legend()\n",
    "    \n",
    "    # Improvement plot\n",
    "    improvements = [1.455 - mae for mae in maes]\n",
    "    ax2.barh(models, improvements, color=['green' if imp > 0 else 'red' for imp in improvements])\n",
    "    ax2.axvline(x=0, color='black', linestyle='-')\n",
    "    ax2.set_xlabel('Improvement over Ridge (D)')\n",
    "    ax2.set_title('Improvement Analysis')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "else:\n",
    "    print(\"Not enough data for advanced analysis\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2df8fa3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 13: Feature Selection and Hyperparameter Optimization - DEBUG VERSION\n",
    "# This version shows progress and runs faster\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import LeaveOneOut, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler, RobustScaler\n",
    "from sklearn.feature_selection import SelectKBest, f_regression, mutual_info_regression\n",
    "from sklearn.linear_model import Ridge, Lasso, ElasticNet, HuberRegressor, BayesianRidge\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, ExtraTreesRegressor\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.pipeline import Pipeline\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm  # For progress bars\n",
    "\n",
    "if len(df_ml) > 10:\n",
    "    # Prepare data\n",
    "    features_ml = ['Bio-AL', 'K_avg_Kerato', 'Posterior Km', 'CCT', 'Post_Ant_Ratio', \n",
    "                   'K_Astigmatism_Kerato', 'A-Constant']\n",
    "    \n",
    "    df_ml_full = df_ml[features_ml + ['True_IOL']].dropna()\n",
    "    X_ml = df_ml_full[features_ml]\n",
    "    y_ml = df_ml_full['True_IOL']\n",
    "    \n",
    "    print(f\"Optimizing with {len(X_ml)} cases\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # 1. FEATURE IMPORTANCE ANALYSIS\n",
    "    print(\"1. FEATURE IMPORTANCE ANALYSIS\")\n",
    "    print(\"-\"*40)\n",
    "    \n",
    "    # Calculate feature importance using multiple methods\n",
    "    # Random Forest importance\n",
    "    rf_temp = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "    rf_temp.fit(X_ml, y_ml)\n",
    "    rf_importance = rf_temp.feature_importances_\n",
    "    \n",
    "    # Mutual Information\n",
    "    mi_scores = mutual_info_regression(X_ml, y_ml, random_state=42)\n",
    "    \n",
    "    # F-statistic\n",
    "    f_scores = SelectKBest(score_func=f_regression, k='all').fit(X_ml, y_ml).scores_\n",
    "    \n",
    "    # Create importance dataframe\n",
    "    importance_df = pd.DataFrame({\n",
    "        'Feature': features_ml,\n",
    "        'RF_Importance': rf_importance,\n",
    "        'MI_Score': mi_scores,\n",
    "        'F_Score': f_scores\n",
    "    })\n",
    "    \n",
    "    # Normalize scores\n",
    "    for col in ['RF_Importance', 'MI_Score', 'F_Score']:\n",
    "        importance_df[f'{col}_norm'] = importance_df[col] / importance_df[col].max()\n",
    "    \n",
    "    # Average normalized importance\n",
    "    importance_df['Avg_Importance'] = importance_df[['RF_Importance_norm', 'MI_Score_norm', 'F_Score_norm']].mean(axis=1)\n",
    "    importance_df = importance_df.sort_values('Avg_Importance', ascending=False)\n",
    "    \n",
    "    print(importance_df[['Feature', 'Avg_Importance']])\n",
    "    \n",
    "    # 2. FEATURE ENGINEERING\n",
    "    print(\"\\n2. ADVANCED FEATURE ENGINEERING\")\n",
    "    print(\"-\"*40)\n",
    "    \n",
    "    # Create new features\n",
    "    X_engineered = X_ml.copy()\n",
    "    \n",
    "    # Add interaction features\n",
    "    X_engineered['AL_K_interaction'] = X_ml['Bio-AL'] * X_ml['K_avg_Kerato']\n",
    "    X_engineered['Posterior_CCT_ratio'] = X_ml['Posterior Km'] / X_ml['CCT']\n",
    "    X_engineered['AL_CCT_ratio'] = X_ml['Bio-AL'] / X_ml['CCT']\n",
    "    X_engineered['K_diff_CCT'] = X_ml['K_Astigmatism_Kerato'] * X_ml['CCT']\n",
    "    \n",
    "    # Add polynomial features for most important variables\n",
    "    X_engineered['AL_squared'] = X_ml['Bio-AL'] ** 2\n",
    "    X_engineered['K_squared'] = X_ml['K_avg_Kerato'] ** 2\n",
    "    X_engineered['Posterior_squared'] = X_ml['Posterior Km'] ** 2\n",
    "    \n",
    "    print(f\"Total features after engineering: {X_engineered.shape[1]}\")\n",
    "    \n",
    "    # 3. SIMPLIFIED HYPERPARAMETER OPTIMIZATION\n",
    "    print(\"\\n3. SIMPLIFIED HYPERPARAMETER OPTIMIZATION\")\n",
    "    print(\"-\"*40)\n",
    "    \n",
    "    # Test fewer configurations for speed\n",
    "    param_grids = {\n",
    "        'Ridge': {\n",
    "            'model__alpha': [0.1, 1.0, 10.0]\n",
    "        },\n",
    "        'Huber': {\n",
    "            'model__epsilon': [1.2, 1.35, 1.5],\n",
    "            'model__alpha': [0.001, 0.01]\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    # Test only RobustScaler\n",
    "    scaler = RobustScaler()\n",
    "    \n",
    "    results_optimized = {}\n",
    "    \n",
    "    # LOO cross-validation\n",
    "    loo = LeaveOneOut()\n",
    "    \n",
    "    # Test with fewer feature combinations\n",
    "    print(\"\\nTesting Ridge with different feature counts...\")\n",
    "    for n_features in [5, 7, X_engineered.shape[1]]:\n",
    "        print(f\"\\nTesting with {n_features} features...\")\n",
    "        \n",
    "        # Create pipeline\n",
    "        if n_features < X_engineered.shape[1]:\n",
    "            pipeline = Pipeline([\n",
    "                ('scaler', scaler),\n",
    "                ('feature_selection', SelectKBest(f_regression, k=n_features)),\n",
    "                ('model', Ridge())\n",
    "            ])\n",
    "        else:\n",
    "            pipeline = Pipeline([\n",
    "                ('scaler', scaler),\n",
    "                ('model', Ridge())\n",
    "            ])\n",
    "        \n",
    "        # Test Ridge with this configuration\n",
    "        predictions = []\n",
    "        actuals = []\n",
    "        \n",
    "        # Show progress\n",
    "        print(f\"Running LOO cross-validation ({len(X_engineered)} iterations)...\")\n",
    "        for i, (train_idx, test_idx) in enumerate(loo.split(X_engineered)):\n",
    "            if i % 20 == 0:\n",
    "                print(f\"Progress: {i}/{len(X_engineered)}\")\n",
    "            \n",
    "            X_train, X_test = X_engineered.iloc[train_idx], X_engineered.iloc[test_idx]\n",
    "            y_train, y_test = y_ml.iloc[train_idx], y_ml.iloc[test_idx]\n",
    "            \n",
    "            # Simple grid search with fewer parameters\n",
    "            grid = GridSearchCV(pipeline, param_grids['Ridge'], cv=3, scoring='neg_mean_absolute_error')\n",
    "            grid.fit(X_train, y_train)\n",
    "            \n",
    "            # Predict on test\n",
    "            predictions.append(grid.predict(X_test)[0])\n",
    "            actuals.append(y_test.values[0])\n",
    "        \n",
    "        mae = mean_absolute_error(actuals, predictions)\n",
    "        config_name = f\"RobustScaler_{n_features}_Features\"\n",
    "        results_optimized[config_name] = mae\n",
    "        print(f\"MAE: {mae:.3f} D\")\n",
    "    \n",
    "    # 4. TEST OTHER MODELS (SIMPLIFIED)\n",
    "    print(\"\\n4. TESTING OTHER MODELS\")\n",
    "    print(\"-\"*40)\n",
    "    \n",
    "    # Select best feature count (usually 7 works well)\n",
    "    n_best_features = 7\n",
    "    selector_best = SelectKBest(f_regression, k=n_best_features)\n",
    "    \n",
    "    # Prepare data with best preprocessing\n",
    "    X_scaled = scaler.fit_transform(X_engineered)\n",
    "    X_selected = selector_best.fit_transform(X_scaled, y_ml)\n",
    "    \n",
    "    # Get selected feature names\n",
    "    selected_features = X_engineered.columns[selector_best.get_support()]\n",
    "    print(f\"\\nSelected top {n_best_features} features: {list(selected_features)}\")\n",
    "    \n",
    "    # Test a few key models\n",
    "    simple_models = {\n",
    "        'Ridge_Optimized': Ridge(alpha=1.0),\n",
    "        'Huber_Optimized': HuberRegressor(epsilon=1.35),\n",
    "        'BayesianRidge_Optimized': BayesianRidge()\n",
    "    }\n",
    "    \n",
    "    for name, model in simple_models.items():\n",
    "        print(f\"\\nTesting {name}...\")\n",
    "        predictions = []\n",
    "        actuals = []\n",
    "        \n",
    "        for train_idx, test_idx in loo.split(X_selected):\n",
    "            X_train, X_test = X_selected[train_idx], X_selected[test_idx]\n",
    "            y_train, y_test = y_ml.iloc[train_idx], y_ml.iloc[test_idx]\n",
    "            \n",
    "            model.fit(X_train, y_train)\n",
    "            predictions.append(model.predict(X_test)[0])\n",
    "            actuals.append(y_test.values[0])\n",
    "        \n",
    "        mae = mean_absolute_error(actuals, predictions)\n",
    "        results_optimized[name] = mae\n",
    "        print(f\"MAE: {mae:.3f} D\")\n",
    "    \n",
    "    # 5. RESULTS SUMMARY\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"OPTIMIZATION RESULTS SUMMARY\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # Sort and display results\n",
    "    sorted_results = sorted(results_optimized.items(), key=lambda x: x[1])\n",
    "    \n",
    "    print(\"\\nAll configurations tested:\")\n",
    "    for i, (config, mae) in enumerate(sorted_results, 1):\n",
    "        improvement = 1.455 - mae\n",
    "        print(f\"{i}. {config}: MAE = {mae:.3f} D (Improvement: {improvement:+.3f} D)\")\n",
    "    \n",
    "    best_config = sorted_results[0][0]\n",
    "    best_mae = sorted_results[0][1]\n",
    "    \n",
    "    print(f\"\\nBEST CONFIGURATION: {best_config}\")\n",
    "    print(f\"MAE: {best_mae:.3f} D\")\n",
    "    print(f\"Improvement over original Ridge: {1.455 - best_mae:.3f} D\")\n",
    "    \n",
    "    # 6. SIMPLE VISUALIZATION\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))\n",
    "    \n",
    "    # Feature importance plot\n",
    "    ax1.barh(importance_df['Feature'][:7], importance_df['Avg_Importance'][:7])\n",
    "    ax1.set_xlabel('Average Normalized Importance')\n",
    "    ax1.set_title('Top 7 Feature Importance')\n",
    "    \n",
    "    # Model comparison\n",
    "    ax2.bar(range(len(sorted_results[:5])), [x[1] for x in sorted_results[:5]])\n",
    "    ax2.axhline(y=1.455, color='red', linestyle='--', label='Original Ridge')\n",
    "    ax2.set_xticks(range(len(sorted_results[:5])))\n",
    "    ax2.set_xticklabels([x[0].split('_')[0] for x in sorted_results[:5]], rotation=45, ha='right')\n",
    "    ax2.set_ylabel('MAE (D)')\n",
    "    ax2.set_title('Top 5 Model Configurations')\n",
    "    ax2.legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"\\nOptimization complete!\")\n",
    "    \n",
    "else:\n",
    "    print(\"Not enough data for optimization\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa59a2f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 14: Specialized Approaches for FacoDMEK Eyes - CORRECTED VERSION\n",
    "# Fixed index alignment issues in LOO cross-validation\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import LeaveOneOut\n",
    "from sklearn.preprocessing import StandardScaler, RobustScaler\n",
    "from sklearn.linear_model import Ridge, LinearRegression\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from scipy import stats\n",
    "from scipy.optimize import minimize\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "if len(df_ml) > 10:\n",
    "    # Prepare base data\n",
    "    features_ml = ['Bio-AL', 'K_avg_Kerato', 'Posterior Km', 'CCT', 'Post_Ant_Ratio', \n",
    "                   'K_Astigmatism_Kerato', 'A-Constant']\n",
    "    \n",
    "    df_analysis = df_ml[features_ml + ['True_IOL', 'SRKT_Prediction', 'SRKT_Error']].dropna().copy()\n",
    "    \n",
    "    print(f\"Analyzing {len(df_analysis)} FacoDMEK eyes\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # 1. ANALYZE SYSTEMATIC BIAS IN SRK/T\n",
    "    print(\"1. SYSTEMATIC BIAS ANALYSIS\")\n",
    "    print(\"-\"*40)\n",
    "    \n",
    "    # Group by key characteristics\n",
    "    # Axial length groups\n",
    "    df_analysis['AL_group'] = pd.cut(df_analysis['Bio-AL'], \n",
    "                                     bins=[0, 22, 24, 26, 100], \n",
    "                                     labels=['Short', 'Normal', 'Long', 'Very Long'])\n",
    "    \n",
    "    # Posterior K groups\n",
    "    df_analysis['PostK_group'] = pd.cut(df_analysis['Posterior Km'], \n",
    "                                        bins=[-10, -7, -6, -5, 0], \n",
    "                                        labels=['Very Steep', 'Steep', 'Normal', 'Flat'])\n",
    "    \n",
    "    # Calculate bias by groups\n",
    "    print(\"\\nMean SRK/T Error by Axial Length:\")\n",
    "    al_bias = df_analysis.groupby('AL_group')['SRKT_Error'].agg(['mean', 'std', 'count'])\n",
    "    print(al_bias)\n",
    "    \n",
    "    print(\"\\nMean SRK/T Error by Posterior K:\")\n",
    "    postk_bias = df_analysis.groupby('PostK_group')['SRKT_Error'].agg(['mean', 'std', 'count'])\n",
    "    print(postk_bias)\n",
    "    \n",
    "    # 2. SEGMENTED CORRECTION MODELS - FIXED VERSION\n",
    "    print(\"\\n2. SEGMENTED CORRECTION APPROACH\")\n",
    "    print(\"-\"*40)\n",
    "    \n",
    "    # Create different models for different eye characteristics\n",
    "    loo = LeaveOneOut()\n",
    "    segmented_predictions = []\n",
    "    actuals = []\n",
    "    model_used = []\n",
    "    \n",
    "    # Reset index to ensure proper alignment\n",
    "    df_analysis_reset = df_analysis.reset_index(drop=True)\n",
    "    \n",
    "    for train_idx, test_idx in loo.split(df_analysis_reset):\n",
    "        train_data = df_analysis_reset.iloc[train_idx].copy()\n",
    "        test_data = df_analysis_reset.iloc[test_idx]\n",
    "        \n",
    "        test_al = test_data['Bio-AL'].values[0]\n",
    "        test_postk = test_data['Posterior Km'].values[0]\n",
    "        \n",
    "        # Determine which segment the test eye belongs to\n",
    "        if test_al < 23:\n",
    "            segment = 'short'\n",
    "            # Create mask using values, not boolean indexing\n",
    "            segment_indices = train_data[train_data['Bio-AL'] < 23.5].index\n",
    "        elif test_al > 25:\n",
    "            segment = 'long'\n",
    "            segment_indices = train_data[train_data['Bio-AL'] > 24.5].index\n",
    "        else:\n",
    "            segment = 'normal'\n",
    "            segment_indices = train_data.index\n",
    "        \n",
    "        # If not enough eyes in segment, use all\n",
    "        if len(segment_indices) < 5:\n",
    "            segment_indices = train_data.index\n",
    "            segment = 'all'\n",
    "        \n",
    "        # Train correction model for this segment\n",
    "        X_segment = train_data.loc[segment_indices, ['Posterior Km', 'CCT', 'Post_Ant_Ratio']]\n",
    "        y_segment = train_data.loc[segment_indices, 'SRKT_Error']\n",
    "        \n",
    "        # Apply RobustScaler\n",
    "        scaler_segment = RobustScaler()\n",
    "        \n",
    "        if len(X_segment) > 3:\n",
    "            X_segment_scaled = scaler_segment.fit_transform(X_segment)\n",
    "            model = Ridge(alpha=1.0)\n",
    "            model.fit(X_segment_scaled, y_segment)\n",
    "            \n",
    "            # Predict correction\n",
    "            X_test = test_data[['Posterior Km', 'CCT', 'Post_Ant_Ratio']].values.reshape(1, -1)\n",
    "            X_test_scaled = scaler_segment.transform(X_test)\n",
    "            correction = model.predict(X_test_scaled)[0]\n",
    "        else:\n",
    "            # Fall back to mean correction\n",
    "            correction = y_segment.mean()\n",
    "        \n",
    "        # Apply correction\n",
    "        srkt_pred = test_data['SRKT_Prediction'].values[0]\n",
    "        final_pred = srkt_pred - correction\n",
    "        \n",
    "        segmented_predictions.append(final_pred)\n",
    "        actuals.append(test_data['True_IOL'].values[0])\n",
    "        model_used.append(segment)\n",
    "    \n",
    "    mae_segmented = mean_absolute_error(actuals, segmented_predictions)\n",
    "    print(f\"Segmented Model MAE: {mae_segmented:.3f} D\")\n",
    "    \n",
    "    # 3. POSTERIOR K-WEIGHTED CORRECTION\n",
    "    print(\"\\n3. POSTERIOR K-WEIGHTED CORRECTION\")\n",
    "    print(\"-\"*40)\n",
    "    \n",
    "    # The idea: weight the correction based on how abnormal the posterior K is\n",
    "    mean_post_k = df_analysis_reset['Posterior Km'].mean()\n",
    "    std_post_k = df_analysis_reset['Posterior Km'].std()\n",
    "    \n",
    "    weighted_predictions = []\n",
    "    \n",
    "    for train_idx, test_idx in loo.split(df_analysis_reset):\n",
    "        train_data = df_analysis_reset.iloc[train_idx]\n",
    "        test_data = df_analysis_reset.iloc[test_idx]\n",
    "        \n",
    "        # Calculate how abnormal the test eye's posterior K is\n",
    "        test_post_k = test_data['Posterior Km'].values[0]\n",
    "        z_score = abs((test_post_k - mean_post_k) / std_post_k)\n",
    "        \n",
    "        # Weight factor (higher weight for more abnormal eyes)\n",
    "        weight = 1 + (z_score * 0.5)  # Adjust multiplier as needed\n",
    "        \n",
    "        # Train model with RobustScaler\n",
    "        X_train = train_data[['Posterior Km', 'CCT']]\n",
    "        y_train = train_data['SRKT_Error']\n",
    "        \n",
    "        scaler_weighted = RobustScaler()\n",
    "        X_train_scaled = scaler_weighted.fit_transform(X_train)\n",
    "        \n",
    "        model = Ridge(alpha=0.5)\n",
    "        model.fit(X_train_scaled, y_train)\n",
    "        \n",
    "        # Predict and apply weighted correction\n",
    "        X_test = test_data[['Posterior Km', 'CCT']].values.reshape(1, -1)\n",
    "        X_test_scaled = scaler_weighted.transform(X_test)\n",
    "        correction = model.predict(X_test_scaled)[0] * weight\n",
    "        \n",
    "        srkt_pred = test_data['SRKT_Prediction'].values[0]\n",
    "        final_pred = srkt_pred - correction\n",
    "        \n",
    "        weighted_predictions.append(final_pred)\n",
    "    \n",
    "    mae_weighted = mean_absolute_error(actuals, weighted_predictions)\n",
    "    print(f\"Posterior K-Weighted Model MAE: {mae_weighted:.3f} D\")\n",
    "    \n",
    "    # 4. RATIO-BASED CORRECTION MODEL\n",
    "    print(\"\\n4. RATIO-BASED CORRECTION MODEL\")\n",
    "    print(\"-\"*40)\n",
    "    \n",
    "    # Create ratio features that might better capture FacoDMEK characteristics\n",
    "    df_ratios = df_analysis_reset.copy()\n",
    "    df_ratios['K_to_AL_ratio'] = df_ratios['K_avg_Kerato'] / df_ratios['Bio-AL']\n",
    "    df_ratios['PostK_to_K_ratio'] = df_ratios['Posterior Km'] / df_ratios['K_avg_Kerato']\n",
    "    df_ratios['CCT_to_AL_ratio'] = df_ratios['CCT'] / df_ratios['Bio-AL']\n",
    "    df_ratios['Corneal_contribution'] = (df_ratios['K_avg_Kerato'] - 43.5) * 0.8  # Deviation from normal\n",
    "    \n",
    "    ratio_features = ['K_to_AL_ratio', 'PostK_to_K_ratio', 'CCT_to_AL_ratio', \n",
    "                     'Corneal_contribution', 'Post_Ant_Ratio']\n",
    "    \n",
    "    ratio_predictions = []\n",
    "    \n",
    "    for train_idx, test_idx in loo.split(df_ratios):\n",
    "        X_train = df_ratios.iloc[train_idx][ratio_features]\n",
    "        X_test = df_ratios.iloc[test_idx][ratio_features]\n",
    "        y_train = df_ratios.iloc[train_idx]['True_IOL']\n",
    "        \n",
    "        # Apply RobustScaler\n",
    "        scaler_ratio = RobustScaler()\n",
    "        X_train_scaled = scaler_ratio.fit_transform(X_train)\n",
    "        X_test_scaled = scaler_ratio.transform(X_test.values.reshape(1, -1))\n",
    "        \n",
    "        model = Ridge(alpha=1.0)\n",
    "        model.fit(X_train_scaled, y_train)\n",
    "        \n",
    "        pred = model.predict(X_test_scaled)[0]\n",
    "        ratio_predictions.append(pred)\n",
    "    \n",
    "    mae_ratio = mean_absolute_error(actuals, ratio_predictions)\n",
    "    print(f\"Ratio-Based Model MAE: {mae_ratio:.3f} D\")\n",
    "    \n",
    "    # 5. CORRECTION FORMULA OPTIMIZATION\n",
    "    print(\"\\n5. OPTIMIZED CORRECTION FORMULA\")\n",
    "    print(\"-\"*40)\n",
    "    \n",
    "    # Optimize a simple correction formula: IOL = SRK/T + a*PostK + b*CCT + c*AL + d\n",
    "    def correction_formula(params, X, y_true, srkt_pred):\n",
    "        a, b, c, d = params\n",
    "        correction = a * X[:, 0] + b * X[:, 1] + c * X[:, 2] + d\n",
    "        predictions = srkt_pred - correction\n",
    "        return mean_absolute_error(y_true, predictions)\n",
    "    \n",
    "    # Prepare data for optimization\n",
    "    X_opt = df_analysis_reset[['Posterior Km', 'CCT', 'Bio-AL']].values\n",
    "    y_true = df_analysis_reset['True_IOL'].values\n",
    "    srkt_pred = df_analysis_reset['SRKT_Prediction'].values\n",
    "    \n",
    "    # Optimize\n",
    "    initial_params = [0.1, 0.001, 0.1, 0]\n",
    "    result = minimize(correction_formula, initial_params, \n",
    "                     args=(X_opt, y_true, srkt_pred),\n",
    "                     method='Nelder-Mead')\n",
    "    \n",
    "    optimal_params = result.x\n",
    "    \n",
    "    # Test with LOO\n",
    "    optimized_predictions = []\n",
    "    \n",
    "    for train_idx, test_idx in loo.split(df_analysis_reset):\n",
    "        X_train = df_analysis_reset.iloc[train_idx][['Posterior Km', 'CCT', 'Bio-AL']].values\n",
    "        y_train = df_analysis_reset.iloc[train_idx]['True_IOL'].values\n",
    "        srkt_train = df_analysis_reset.iloc[train_idx]['SRKT_Prediction'].values\n",
    "        \n",
    "        # Re-optimize on training set\n",
    "        result_cv = minimize(correction_formula, initial_params, \n",
    "                           args=(X_train, y_train, srkt_train),\n",
    "                           method='Nelder-Mead')\n",
    "        \n",
    "        # Apply to test\n",
    "        test_data = df_analysis_reset.iloc[test_idx]\n",
    "        X_test = test_data[['Posterior Km', 'CCT', 'Bio-AL']].values\n",
    "        \n",
    "        a, b, c, d = result_cv.x\n",
    "        correction = a * X_test[0] + b * X_test[1] + c * X_test[2] + d\n",
    "        \n",
    "        final_pred = test_data['SRKT_Prediction'].values[0] - correction\n",
    "        optimized_predictions.append(final_pred)\n",
    "    \n",
    "    mae_optimized = mean_absolute_error(actuals, optimized_predictions)\n",
    "    print(f\"Optimized Formula MAE: {mae_optimized:.3f} D\")\n",
    "    \n",
    "    print(f\"\\nOptimized Correction Formula:\")\n",
    "    print(f\"IOL = SRK/T - ({optimal_params[0]:.4f} × Posterior_K + \"\n",
    "          f\"{optimal_params[1]:.6f} × CCT + {optimal_params[2]:.4f} × AL + {optimal_params[3]:.3f})\")\n",
    "    \n",
    "    # 6. RESULTS COMPARISON\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"SPECIALIZED MODELS COMPARISON\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    results_specialized = {\n",
    "        'Original SRK/T': df_analysis_reset['SRKT_Error'].abs().mean(),\n",
    "        'Segmented Model': mae_segmented,\n",
    "        'Posterior K-Weighted': mae_weighted,\n",
    "        'Ratio-Based Model': mae_ratio,\n",
    "        'Optimized Formula': mae_optimized\n",
    "    }\n",
    "    \n",
    "    # Sort by performance\n",
    "    sorted_specialized = sorted(results_specialized.items(), key=lambda x: x[1])\n",
    "    \n",
    "    print(\"\\nModel Performance:\")\n",
    "    for model, mae in sorted_specialized:\n",
    "        improvement = 1.455 - mae\n",
    "        print(f\"{model}: MAE = {mae:.3f} D (Improvement: {improvement:+.3f} D)\")\n",
    "    \n",
    "    # Visualization\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "    \n",
    "    # Error by AL group\n",
    "    ax1 = axes[0, 0]\n",
    "    df_analysis_reset.boxplot(column='SRKT_Error', by='AL_group', ax=ax1)\n",
    "    ax1.set_xlabel('Axial Length Group')\n",
    "    ax1.set_ylabel('SRK/T Error (D)')\n",
    "    ax1.set_title('SRK/T Error Distribution by AL Group')\n",
    "    ax1.axhline(y=0, color='red', linestyle='--')\n",
    "    \n",
    "    # Error vs Posterior K\n",
    "    ax2 = axes[0, 1]\n",
    "    scatter = ax2.scatter(df_analysis_reset['Posterior Km'], df_analysis_reset['SRKT_Error'], \n",
    "                         c=df_analysis_reset['Bio-AL'], cmap='viridis', alpha=0.6)\n",
    "    ax2.set_xlabel('Posterior K (D)')\n",
    "    ax2.set_ylabel('SRK/T Error (D)')\n",
    "    ax2.set_title('Error vs Posterior K (colored by AL)')\n",
    "    ax2.axhline(y=0, color='red', linestyle='--')\n",
    "    plt.colorbar(scatter, ax=ax2, label='Axial Length (mm)')\n",
    "    \n",
    "    # Model comparison\n",
    "    ax3 = axes[1, 0]\n",
    "    models = list(results_specialized.keys())\n",
    "    maes = list(results_specialized.values())\n",
    "    bars = ax3.bar(range(len(models)), maes, color=['red', 'orange', 'yellow', 'lightgreen', 'green'])\n",
    "    ax3.set_xticks(range(len(models)))\n",
    "    ax3.set_xticklabels(models, rotation=45, ha='right')\n",
    "    ax3.set_ylabel('MAE (D)')\n",
    "    ax3.set_title('Specialized Model Comparison')\n",
    "    ax3.axhline(y=1.455, color='blue', linestyle='--', label='Original Ridge')\n",
    "    ax3.legend()\n",
    "    \n",
    "    # Best model error distribution\n",
    "    ax4 = axes[1, 1]\n",
    "    best_errors = np.array(optimized_predictions) - np.array(actuals)\n",
    "    ax4.hist(best_errors, bins=15, edgecolor='black', alpha=0.7, color='green')\n",
    "    ax4.axvline(x=0, color='red', linestyle='--')\n",
    "    ax4.set_xlabel('Prediction Error (D)')\n",
    "    ax4.set_ylabel('Frequency')\n",
    "    ax4.set_title(f'Optimized Formula Error Distribution (MAE: {mae_optimized:.3f} D)')\n",
    "    \n",
    "    # Add normal distribution overlay\n",
    "    mu, std = stats.norm.fit(best_errors)\n",
    "    x = np.linspace(best_errors.min(), best_errors.max(), 100)\n",
    "    ax4.plot(x, stats.norm.pdf(x, mu, std) * len(best_errors) * (best_errors.max() - best_errors.min()) / 15, \n",
    "             'r-', linewidth=2, label=f'Normal fit (μ={mu:.2f}, σ={std:.2f})')\n",
    "    ax4.legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Final recommendation\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"RECOMMENDATION FOR CLINICAL USE\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    best_model = sorted_specialized[0][0]\n",
    "    best_mae = sorted_specialized[0][1]\n",
    "    \n",
    "    print(f\"Best performing model: {best_model} with MAE: {best_mae:.3f} D\")\n",
    "    \n",
    "    if best_model == \"Optimized Formula\":\n",
    "        print(\"\\nClinical Formula:\")\n",
    "        print(\"1. Calculate standard SRK/T using keratometry K values\")\n",
    "        print(\"2. Apply correction:\")\n",
    "        print(f\"   Correction = {optimal_params[0]:.3f} × Posterior_K + \"\n",
    "              f\"{optimal_params[1]:.5f} × CCT + {optimal_params[2]:.3f} × AL + {optimal_params[3]:.2f}\")\n",
    "        print(\"3. Modified IOL = SRK/T - Correction\")\n",
    "        \n",
    "    # Calculate final accuracy metrics\n",
    "    errors_abs = np.abs(best_errors)\n",
    "    print(f\"\\nAccuracy with {best_model}:\")\n",
    "    print(f\"Within ±0.25 D: {(errors_abs <= 0.25).sum() / len(errors_abs) * 100:.1f}%\")\n",
    "    print(f\"Within ±0.50 D: {(errors_abs <= 0.50).sum() / len(errors_abs) * 100:.1f}%\")\n",
    "    print(f\"Within ±1.00 D: {(errors_abs <= 1.00).sum() / len(errors_abs) * 100:.1f}%\")\n",
    "    \n",
    "else:\n",
    "    print(\"Not enough data for specialized analysis\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
